/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-10701
11/15/2024 13:53:02 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 13:53:05 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:17,  4.04it/s]  4%|▍         | 3/73 [00:00<00:24,  2.84it/s]  5%|▌         | 4/73 [00:01<00:27,  2.48it/s]  7%|▋         | 5/73 [00:01<00:28,  2.35it/s]  8%|▊         | 6/73 [00:02<00:29,  2.30it/s] 10%|▉         | 7/73 [00:02<00:29,  2.26it/s] 11%|█         | 8/73 [00:03<00:28,  2.24it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.23it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.20it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.19it/s] 16%|█▋        | 12/73 [00:05<00:27,  2.19it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.19it/s] 19%|█▉        | 14/73 [00:06<00:26,  2.20it/s] 21%|██        | 15/73 [00:06<00:26,  2.19it/s] 22%|██▏       | 16/73 [00:06<00:26,  2.19it/s] 23%|██▎       | 17/73 [00:07<00:25,  2.16it/s] 25%|██▍       | 18/73 [00:07<00:25,  2.17it/s] 26%|██▌       | 19/73 [00:08<00:24,  2.17it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.18it/s] 29%|██▉       | 21/73 [00:09<00:23,  2.18it/s] 30%|███       | 22/73 [00:09<00:23,  2.18it/s] 32%|███▏      | 23/73 [00:10<00:22,  2.19it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.15it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.13it/s] 36%|███▌      | 26/73 [00:11<00:22,  2.08it/s] 37%|███▋      | 27/73 [00:12<00:22,  2.07it/s] 38%|███▊      | 28/73 [00:12<00:21,  2.09it/s] 40%|███▉      | 29/73 [00:13<00:20,  2.12it/s] 41%|████      | 30/73 [00:13<00:20,  2.15it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.16it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.18it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.18it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.19it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.19it/s] 49%|████▉     | 36/73 [00:16<00:16,  2.20it/s] 51%|█████     | 37/73 [00:16<00:16,  2.20it/s] 52%|█████▏    | 38/73 [00:17<00:15,  2.20it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.20it/s] 55%|█████▍    | 40/73 [00:18<00:14,  2.20it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.20it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.20it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.20it/s] 60%|██████    | 44/73 [00:19<00:13,  2.18it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.19it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.19it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.19it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.20it/s] 67%|██████▋   | 49/73 [00:22<00:10,  2.20it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.20it/s] 70%|██████▉   | 51/73 [00:23<00:10,  2.20it/s] 71%|███████   | 52/73 [00:23<00:09,  2.17it/s] 73%|███████▎  | 53/73 [00:24<00:09,  2.18it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.18it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.19it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.19it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.20it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.20it/s] 81%|████████  | 59/73 [00:26<00:06,  2.20it/s] 82%|████████▏ | 60/73 [00:27<00:05,  2.20it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.20it/s] 85%|████████▍ | 62/73 [00:28<00:04,  2.20it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.20it/s] 88%|████████▊ | 64/73 [00:29<00:04,  2.20it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.20it/s] 90%|█████████ | 66/73 [00:29<00:03,  2.20it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.20it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.20it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.20it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.20it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.20it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.21it/s]100%|██████████| 73/73 [00:32<00:00,  2.64it/s]100%|██████████| 73/73 [00:33<00:00,  2.20it/s]
***** eval metrics *****
  eval_accuracy               =     0.6586
  eval_loss                   =     1.9472
  eval_model_preparation_time =     0.0029
  eval_runtime                = 0:00:34.03
  eval_samples                =        580
  eval_samples_per_second     =      17.04
  eval_steps_per_second       =      2.145
  perplexity                  =     7.0092
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-1189
11/15/2024 13:53:47 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 13:53:50 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:17,  4.08it/s]  4%|▍         | 3/73 [00:00<00:24,  2.87it/s]  5%|▌         | 4/73 [00:01<00:27,  2.53it/s]  7%|▋         | 5/73 [00:01<00:28,  2.39it/s]  8%|▊         | 6/73 [00:02<00:28,  2.33it/s] 10%|▉         | 7/73 [00:02<00:29,  2.27it/s] 11%|█         | 8/73 [00:03<00:28,  2.25it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.23it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.19it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.19it/s] 16%|█▋        | 12/73 [00:05<00:27,  2.19it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.20it/s] 19%|█▉        | 14/73 [00:06<00:26,  2.20it/s] 21%|██        | 15/73 [00:06<00:26,  2.20it/s] 22%|██▏       | 16/73 [00:06<00:25,  2.20it/s] 23%|██▎       | 17/73 [00:07<00:25,  2.18it/s] 25%|██▍       | 18/73 [00:07<00:25,  2.19it/s] 26%|██▌       | 19/73 [00:08<00:24,  2.16it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.17it/s] 29%|██▉       | 21/73 [00:09<00:23,  2.18it/s] 30%|███       | 22/73 [00:09<00:23,  2.19it/s] 32%|███▏      | 23/73 [00:10<00:22,  2.19it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.20it/s] 34%|███▍      | 25/73 [00:11<00:21,  2.20it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.20it/s] 37%|███▋      | 27/73 [00:11<00:20,  2.20it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.20it/s] 40%|███▉      | 29/73 [00:12<00:19,  2.20it/s] 41%|████      | 30/73 [00:13<00:19,  2.20it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.18it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.19it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.16it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.17it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.17it/s] 49%|████▉     | 36/73 [00:16<00:17,  2.18it/s] 51%|█████     | 37/73 [00:16<00:16,  2.18it/s] 52%|█████▏    | 38/73 [00:17<00:15,  2.19it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.19it/s] 55%|█████▍    | 40/73 [00:17<00:15,  2.20it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.20it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.20it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.20it/s] 60%|██████    | 44/73 [00:19<00:13,  2.20it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.20it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.20it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.20it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.20it/s] 67%|██████▋   | 49/73 [00:22<00:10,  2.20it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.20it/s] 70%|██████▉   | 51/73 [00:22<00:09,  2.20it/s] 71%|███████   | 52/73 [00:23<00:09,  2.21it/s] 73%|███████▎  | 53/73 [00:23<00:09,  2.20it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.21it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.20it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.20it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.20it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.19it/s] 81%|████████  | 59/73 [00:26<00:06,  2.19it/s] 82%|████████▏ | 60/73 [00:27<00:05,  2.20it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.20it/s] 85%|████████▍ | 62/73 [00:27<00:05,  2.20it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.20it/s] 88%|████████▊ | 64/73 [00:28<00:04,  2.17it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.17it/s] 90%|█████████ | 66/73 [00:29<00:03,  2.18it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.18it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.19it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.19it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.20it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.20it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.21it/s]100%|██████████| 73/73 [00:32<00:00,  2.64it/s]100%|██████████| 73/73 [00:32<00:00,  2.21it/s]
***** eval metrics *****
  eval_accuracy               =     0.5212
  eval_loss                   =     3.0146
  eval_model_preparation_time =      0.004
  eval_runtime                = 0:00:33.89
  eval_samples                =        580
  eval_samples_per_second     =     17.111
  eval_steps_per_second       =      2.154
  perplexity                  =     20.381
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-11890
11/15/2024 13:54:32 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 13:54:35 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:16,  4.20it/s]  4%|▍         | 3/73 [00:00<00:23,  2.96it/s]  5%|▌         | 4/73 [00:01<00:26,  2.60it/s]  7%|▋         | 5/73 [00:01<00:28,  2.42it/s]  8%|▊         | 6/73 [00:02<00:28,  2.33it/s] 10%|▉         | 7/73 [00:02<00:29,  2.26it/s] 11%|█         | 8/73 [00:03<00:29,  2.22it/s] 12%|█▏        | 9/73 [00:03<00:29,  2.20it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.18it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.16it/s] 16%|█▋        | 12/73 [00:05<00:28,  2.13it/s] 18%|█▊        | 13/73 [00:05<00:28,  2.14it/s] 19%|█▉        | 14/73 [00:06<00:27,  2.15it/s] 21%|██        | 15/73 [00:06<00:26,  2.15it/s] 22%|██▏       | 16/73 [00:07<00:26,  2.15it/s] 23%|██▎       | 17/73 [00:07<00:26,  2.14it/s] 25%|██▍       | 18/73 [00:07<00:25,  2.12it/s] 26%|██▌       | 19/73 [00:08<00:25,  2.12it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.14it/s] 29%|██▉       | 21/73 [00:09<00:24,  2.14it/s] 30%|███       | 22/73 [00:09<00:23,  2.15it/s] 32%|███▏      | 23/73 [00:10<00:23,  2.15it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.16it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.16it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.16it/s] 37%|███▋      | 27/73 [00:12<00:21,  2.16it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.17it/s] 40%|███▉      | 29/73 [00:13<00:20,  2.17it/s] 41%|████      | 30/73 [00:13<00:19,  2.17it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.17it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.17it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.17it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.17it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.17it/s] 49%|████▉     | 36/73 [00:16<00:17,  2.17it/s] 51%|█████     | 37/73 [00:16<00:16,  2.16it/s] 52%|█████▏    | 38/73 [00:17<00:16,  2.17it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.14it/s] 55%|█████▍    | 40/73 [00:18<00:15,  2.16it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.17it/s] 58%|█████▊    | 42/73 [00:19<00:14,  2.18it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.18it/s] 60%|██████    | 44/73 [00:19<00:13,  2.19it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.19it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.19it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.19it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.20it/s] 67%|██████▋   | 49/73 [00:22<00:10,  2.20it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.20it/s] 70%|██████▉   | 51/73 [00:23<00:10,  2.20it/s] 71%|███████   | 52/73 [00:23<00:09,  2.20it/s] 73%|███████▎  | 53/73 [00:24<00:09,  2.20it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.20it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.20it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.20it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.20it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.20it/s] 81%|████████  | 59/73 [00:26<00:06,  2.20it/s] 82%|████████▏ | 60/73 [00:27<00:05,  2.20it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.20it/s] 85%|████████▍ | 62/73 [00:28<00:05,  2.20it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.20it/s] 88%|████████▊ | 64/73 [00:29<00:04,  2.20it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.19it/s] 90%|█████████ | 66/73 [00:29<00:03,  2.18it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.18it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.19it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.19it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.19it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.19it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.21it/s]100%|██████████| 73/73 [00:32<00:00,  2.64it/s]100%|██████████| 73/73 [00:33<00:00,  2.20it/s]
***** eval metrics *****
  eval_accuracy               =     0.6794
  eval_loss                   =     1.8023
  eval_model_preparation_time =     0.0036
  eval_runtime                = 0:00:34.08
  eval_samples                =        580
  eval_samples_per_second     =     17.017
  eval_steps_per_second       =      2.142
  perplexity                  =     6.0633
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-13079
11/15/2024 13:55:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 13:55:21 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:16,  4.30it/s]  4%|▍         | 3/73 [00:00<00:23,  3.04it/s]  5%|▌         | 4/73 [00:01<00:25,  2.66it/s]  7%|▋         | 5/73 [00:01<00:27,  2.47it/s]  8%|▊         | 6/73 [00:02<00:28,  2.38it/s] 10%|▉         | 7/73 [00:02<00:28,  2.32it/s] 11%|█         | 8/73 [00:03<00:28,  2.28it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.22it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.21it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.20it/s] 16%|█▋        | 12/73 [00:05<00:27,  2.20it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.20it/s] 19%|█▉        | 14/73 [00:05<00:26,  2.20it/s] 21%|██        | 15/73 [00:06<00:26,  2.20it/s] 22%|██▏       | 16/73 [00:06<00:25,  2.20it/s] 23%|██▎       | 17/73 [00:07<00:25,  2.20it/s] 25%|██▍       | 18/73 [00:07<00:25,  2.20it/s] 26%|██▌       | 19/73 [00:08<00:24,  2.20it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.20it/s] 29%|██▉       | 21/73 [00:09<00:23,  2.19it/s] 30%|███       | 22/73 [00:09<00:23,  2.20it/s] 32%|███▏      | 23/73 [00:10<00:22,  2.20it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.20it/s] 34%|███▍      | 25/73 [00:10<00:21,  2.20it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.20it/s] 37%|███▋      | 27/73 [00:11<00:21,  2.18it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.19it/s] 40%|███▉      | 29/73 [00:12<00:20,  2.19it/s] 41%|████      | 30/73 [00:13<00:19,  2.19it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.19it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.19it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.20it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.17it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.15it/s] 49%|████▉     | 36/73 [00:16<00:17,  2.12it/s] 51%|█████     | 37/73 [00:16<00:17,  2.11it/s] 52%|█████▏    | 38/73 [00:17<00:16,  2.10it/s] 53%|█████▎    | 39/73 [00:17<00:16,  2.07it/s] 55%|█████▍    | 40/73 [00:17<00:15,  2.08it/s] 56%|█████▌    | 41/73 [00:18<00:15,  2.08it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.08it/s] 59%|█████▉    | 43/73 [00:19<00:14,  2.05it/s] 60%|██████    | 44/73 [00:19<00:14,  2.07it/s] 62%|██████▏   | 45/73 [00:20<00:13,  2.08it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.10it/s] 64%|██████▍   | 47/73 [00:21<00:12,  2.11it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.11it/s] 67%|██████▋   | 49/73 [00:22<00:11,  2.12it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.13it/s] 70%|██████▉   | 51/73 [00:23<00:10,  2.12it/s] 71%|███████   | 52/73 [00:23<00:09,  2.13it/s] 73%|███████▎  | 53/73 [00:24<00:09,  2.14it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.15it/s] 75%|███████▌  | 55/73 [00:25<00:08,  2.16it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.17it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.18it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.19it/s] 81%|████████  | 59/73 [00:26<00:06,  2.19it/s] 82%|████████▏ | 60/73 [00:27<00:05,  2.19it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.19it/s] 85%|████████▍ | 62/73 [00:28<00:05,  2.19it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.19it/s] 88%|████████▊ | 64/73 [00:29<00:04,  2.20it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.20it/s] 90%|█████████ | 66/73 [00:30<00:03,  2.20it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.20it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.18it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.19it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.19it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.19it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.21it/s]100%|██████████| 73/73 [00:33<00:00,  2.64it/s]100%|██████████| 73/73 [00:33<00:00,  2.19it/s]
***** eval metrics *****
  eval_accuracy               =      0.686
  eval_loss                   =     1.7675
  eval_model_preparation_time =     0.0029
  eval_runtime                = 0:00:34.13
  eval_samples                =        580
  eval_samples_per_second     =     16.989
  eval_steps_per_second       =      2.138
  perplexity                  =      5.856
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-14268
11/15/2024 13:56:03 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 13:56:06 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:16,  4.23it/s]  4%|▍         | 3/73 [00:00<00:23,  2.98it/s]  5%|▌         | 4/73 [00:01<00:26,  2.63it/s]  7%|▋         | 5/73 [00:01<00:27,  2.45it/s]  8%|▊         | 6/73 [00:02<00:28,  2.32it/s] 10%|▉         | 7/73 [00:02<00:29,  2.26it/s] 11%|█         | 8/73 [00:03<00:29,  2.23it/s] 12%|█▏        | 9/73 [00:03<00:29,  2.20it/s] 14%|█▎        | 10/73 [00:04<00:29,  2.16it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.16it/s] 16%|█▋        | 12/73 [00:05<00:28,  2.17it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.18it/s] 19%|█▉        | 14/73 [00:06<00:27,  2.18it/s] 21%|██        | 15/73 [00:06<00:26,  2.18it/s] 22%|██▏       | 16/73 [00:06<00:26,  2.18it/s] 23%|██▎       | 17/73 [00:07<00:25,  2.17it/s] 25%|██▍       | 18/73 [00:07<00:25,  2.17it/s] 26%|██▌       | 19/73 [00:08<00:25,  2.15it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.16it/s] 29%|██▉       | 21/73 [00:09<00:24,  2.16it/s] 30%|███       | 22/73 [00:09<00:23,  2.17it/s] 32%|███▏      | 23/73 [00:10<00:22,  2.18it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.17it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.17it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.18it/s] 37%|███▋      | 27/73 [00:12<00:21,  2.18it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.19it/s] 40%|███▉      | 29/73 [00:12<00:20,  2.19it/s] 41%|████      | 30/73 [00:13<00:19,  2.18it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.16it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.17it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.18it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.18it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.18it/s] 49%|████▉     | 36/73 [00:16<00:16,  2.19it/s] 51%|█████     | 37/73 [00:16<00:16,  2.19it/s] 52%|█████▏    | 38/73 [00:17<00:15,  2.19it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.19it/s] 55%|█████▍    | 40/73 [00:17<00:15,  2.19it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.19it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.19it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.19it/s] 60%|██████    | 44/73 [00:19<00:13,  2.19it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.16it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.16it/s] 64%|██████▍   | 47/73 [00:21<00:12,  2.14it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.15it/s] 67%|██████▋   | 49/73 [00:22<00:11,  2.16it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.17it/s] 70%|██████▉   | 51/73 [00:23<00:10,  2.17it/s] 71%|███████   | 52/73 [00:23<00:09,  2.18it/s] 73%|███████▎  | 53/73 [00:23<00:09,  2.18it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.18it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.16it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.17it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.17it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.18it/s] 81%|████████  | 59/73 [00:26<00:06,  2.18it/s] 82%|████████▏ | 60/73 [00:27<00:06,  2.16it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.16it/s] 85%|████████▍ | 62/73 [00:28<00:05,  2.15it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.15it/s] 88%|████████▊ | 64/73 [00:29<00:04,  2.17it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.14it/s] 90%|█████████ | 66/73 [00:30<00:03,  2.15it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.16it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.17it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.18it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.18it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.18it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.20it/s]100%|██████████| 73/73 [00:32<00:00,  2.63it/s]100%|██████████| 73/73 [00:33<00:00,  2.20it/s]
***** eval metrics *****
  eval_accuracy               =     0.6829
  eval_loss                   =      1.787
  eval_model_preparation_time =     0.0029
  eval_runtime                = 0:00:34.09
  eval_samples                =        580
  eval_samples_per_second     =     17.012
  eval_steps_per_second       =      2.141
  perplexity                  =     5.9716
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-15457
11/15/2024 13:56:49 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 13:56:52 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:17,  4.11it/s]  4%|▍         | 3/73 [00:00<00:24,  2.88it/s]  5%|▌         | 4/73 [00:01<00:28,  2.43it/s]  7%|▋         | 5/73 [00:01<00:29,  2.28it/s]  8%|▊         | 6/73 [00:02<00:30,  2.21it/s] 10%|▉         | 7/73 [00:02<00:30,  2.16it/s] 11%|█         | 8/73 [00:03<00:30,  2.13it/s] 12%|█▏        | 9/73 [00:03<00:30,  2.11it/s] 14%|█▎        | 10/73 [00:04<00:30,  2.09it/s] 15%|█▌        | 11/73 [00:04<00:29,  2.09it/s] 16%|█▋        | 12/73 [00:05<00:29,  2.09it/s] 18%|█▊        | 13/73 [00:05<00:28,  2.08it/s] 19%|█▉        | 14/73 [00:06<00:28,  2.09it/s] 21%|██        | 15/73 [00:06<00:27,  2.08it/s] 22%|██▏       | 16/73 [00:07<00:27,  2.08it/s] 23%|██▎       | 17/73 [00:07<00:26,  2.08it/s] 25%|██▍       | 18/73 [00:08<00:26,  2.09it/s] 26%|██▌       | 19/73 [00:08<00:25,  2.09it/s] 27%|██▋       | 20/73 [00:09<00:25,  2.09it/s] 29%|██▉       | 21/73 [00:09<00:24,  2.09it/s] 30%|███       | 22/73 [00:10<00:24,  2.09it/s] 32%|███▏      | 23/73 [00:10<00:23,  2.10it/s] 33%|███▎      | 24/73 [00:11<00:23,  2.08it/s] 34%|███▍      | 25/73 [00:11<00:23,  2.08it/s] 36%|███▌      | 26/73 [00:12<00:22,  2.09it/s] 37%|███▋      | 27/73 [00:12<00:21,  2.10it/s] 38%|███▊      | 28/73 [00:13<00:21,  2.10it/s] 40%|███▉      | 29/73 [00:13<00:20,  2.11it/s] 41%|████      | 30/73 [00:13<00:20,  2.11it/s] 42%|████▏     | 31/73 [00:14<00:19,  2.11it/s] 44%|████▍     | 32/73 [00:14<00:19,  2.12it/s] 45%|████▌     | 33/73 [00:15<00:18,  2.11it/s] 47%|████▋     | 34/73 [00:15<00:18,  2.11it/s] 48%|████▊     | 35/73 [00:16<00:17,  2.12it/s] 49%|████▉     | 36/73 [00:16<00:17,  2.12it/s] 51%|█████     | 37/73 [00:17<00:16,  2.13it/s] 52%|█████▏    | 38/73 [00:17<00:16,  2.13it/s] 53%|█████▎    | 39/73 [00:18<00:15,  2.13it/s] 55%|█████▍    | 40/73 [00:18<00:15,  2.13it/s] 56%|█████▌    | 41/73 [00:19<00:14,  2.14it/s] 58%|█████▊    | 42/73 [00:19<00:14,  2.14it/s] 59%|█████▉    | 43/73 [00:20<00:14,  2.14it/s] 60%|██████    | 44/73 [00:20<00:13,  2.14it/s] 62%|██████▏   | 45/73 [00:21<00:13,  2.14it/s] 63%|██████▎   | 46/73 [00:21<00:12,  2.14it/s] 64%|██████▍   | 47/73 [00:21<00:12,  2.14it/s] 66%|██████▌   | 48/73 [00:22<00:11,  2.15it/s] 67%|██████▋   | 49/73 [00:22<00:11,  2.15it/s] 68%|██████▊   | 50/73 [00:23<00:10,  2.15it/s] 70%|██████▉   | 51/73 [00:23<00:10,  2.15it/s] 71%|███████   | 52/73 [00:24<00:09,  2.13it/s] 73%|███████▎  | 53/73 [00:24<00:09,  2.14it/s] 74%|███████▍  | 54/73 [00:25<00:08,  2.14it/s] 75%|███████▌  | 55/73 [00:25<00:08,  2.14it/s] 77%|███████▋  | 56/73 [00:26<00:07,  2.15it/s] 78%|███████▊  | 57/73 [00:26<00:07,  2.15it/s] 79%|███████▉  | 58/73 [00:27<00:06,  2.15it/s] 81%|████████  | 59/73 [00:27<00:06,  2.15it/s] 82%|████████▏ | 60/73 [00:27<00:06,  2.15it/s] 84%|████████▎ | 61/73 [00:28<00:05,  2.15it/s] 85%|████████▍ | 62/73 [00:28<00:05,  2.16it/s] 86%|████████▋ | 63/73 [00:29<00:04,  2.15it/s] 88%|████████▊ | 64/73 [00:29<00:04,  2.16it/s] 89%|████████▉ | 65/73 [00:30<00:03,  2.16it/s] 90%|█████████ | 66/73 [00:30<00:03,  2.16it/s] 92%|█████████▏| 67/73 [00:31<00:02,  2.16it/s] 93%|█████████▎| 68/73 [00:31<00:02,  2.17it/s] 95%|█████████▍| 69/73 [00:32<00:01,  2.17it/s] 96%|█████████▌| 70/73 [00:32<00:01,  2.17it/s] 97%|█████████▋| 71/73 [00:33<00:00,  2.17it/s] 99%|█████████▊| 72/73 [00:33<00:00,  2.18it/s]100%|██████████| 73/73 [00:33<00:00,  2.61it/s]100%|██████████| 73/73 [00:34<00:00,  2.15it/s]
***** eval metrics *****
  eval_accuracy               =     0.6936
  eval_loss                   =     1.7206
  eval_model_preparation_time =     0.0028
  eval_runtime                = 0:00:34.92
  eval_samples                =        580
  eval_samples_per_second     =     16.605
  eval_steps_per_second       =       2.09
  perplexity                  =     5.5879
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-16646
11/15/2024 13:57:36 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 13:57:39 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:16,  4.23it/s]  4%|▍         | 3/73 [00:00<00:23,  2.99it/s]  5%|▌         | 4/73 [00:01<00:26,  2.62it/s]  7%|▋         | 5/73 [00:01<00:27,  2.44it/s]  8%|▊         | 6/73 [00:02<00:28,  2.34it/s] 10%|▉         | 7/73 [00:02<00:28,  2.28it/s] 11%|█         | 8/73 [00:03<00:29,  2.24it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.22it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.20it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.16it/s] 16%|█▋        | 12/73 [00:05<00:28,  2.17it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.17it/s] 19%|█▉        | 14/73 [00:06<00:27,  2.17it/s] 21%|██        | 15/73 [00:06<00:26,  2.17it/s] 22%|██▏       | 16/73 [00:06<00:26,  2.18it/s] 23%|██▎       | 17/73 [00:07<00:25,  2.17it/s] 25%|██▍       | 18/73 [00:07<00:25,  2.18it/s] 26%|██▌       | 19/73 [00:08<00:24,  2.17it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.17it/s] 29%|██▉       | 21/73 [00:09<00:23,  2.17it/s] 30%|███       | 22/73 [00:09<00:23,  2.15it/s] 32%|███▏      | 23/73 [00:10<00:23,  2.15it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.16it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.17it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.17it/s] 37%|███▋      | 27/73 [00:12<00:21,  2.17it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.18it/s] 40%|███▉      | 29/73 [00:12<00:20,  2.18it/s] 41%|████      | 30/73 [00:13<00:19,  2.18it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.18it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.18it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.18it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.18it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.16it/s] 49%|████▉     | 36/73 [00:16<00:17,  2.17it/s] 51%|█████     | 37/73 [00:16<00:16,  2.17it/s] 52%|█████▏    | 38/73 [00:17<00:16,  2.18it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.18it/s] 55%|█████▍    | 40/73 [00:18<00:15,  2.18it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.18it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.19it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.18it/s] 60%|██████    | 44/73 [00:19<00:13,  2.19it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.19it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.19it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.19it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.19it/s] 67%|██████▋   | 49/73 [00:22<00:10,  2.19it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.19it/s] 70%|██████▉   | 51/73 [00:23<00:10,  2.19it/s] 71%|███████   | 52/73 [00:23<00:09,  2.19it/s] 73%|███████▎  | 53/73 [00:23<00:09,  2.19it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.19it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.19it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.19it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.19it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.19it/s] 81%|████████  | 59/73 [00:26<00:06,  2.19it/s] 82%|████████▏ | 60/73 [00:27<00:05,  2.19it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.19it/s] 85%|████████▍ | 62/73 [00:28<00:05,  2.19it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.19it/s] 88%|████████▊ | 64/73 [00:28<00:04,  2.19it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.19it/s] 90%|█████████ | 66/73 [00:29<00:03,  2.19it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.19it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.19it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.19it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.16it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.16it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.19it/s]100%|██████████| 73/73 [00:32<00:00,  2.62it/s]100%|██████████| 73/73 [00:33<00:00,  2.20it/s]
***** eval metrics *****
  eval_accuracy               =     0.6889
  eval_loss                   =     1.7474
  eval_model_preparation_time =     0.0037
  eval_runtime                = 0:00:33.99
  eval_samples                =        580
  eval_samples_per_second     =     17.061
  eval_steps_per_second       =      2.147
  perplexity                  =     5.7395
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-17835
11/15/2024 13:58:21 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 13:58:24 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:17,  4.06it/s]  4%|▍         | 3/73 [00:00<00:24,  2.85it/s]  5%|▌         | 4/73 [00:01<00:27,  2.47it/s]  7%|▋         | 5/73 [00:01<00:29,  2.34it/s]  8%|▊         | 6/73 [00:02<00:29,  2.29it/s] 10%|▉         | 7/73 [00:02<00:29,  2.26it/s] 11%|█         | 8/73 [00:03<00:29,  2.24it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.22it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.21it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.21it/s] 16%|█▋        | 12/73 [00:05<00:27,  2.21it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.20it/s] 19%|█▉        | 14/73 [00:06<00:26,  2.20it/s] 21%|██        | 15/73 [00:06<00:26,  2.20it/s] 22%|██▏       | 16/73 [00:06<00:25,  2.20it/s] 23%|██▎       | 17/73 [00:07<00:25,  2.18it/s] 25%|██▍       | 18/73 [00:07<00:25,  2.19it/s] 26%|██▌       | 19/73 [00:08<00:24,  2.19it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.19it/s] 29%|██▉       | 21/73 [00:09<00:23,  2.18it/s] 30%|███       | 22/73 [00:09<00:23,  2.18it/s] 32%|███▏      | 23/73 [00:10<00:22,  2.18it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.19it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.16it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.17it/s] 37%|███▋      | 27/73 [00:12<00:21,  2.17it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.18it/s] 40%|███▉      | 29/73 [00:12<00:20,  2.19it/s] 41%|████      | 30/73 [00:13<00:19,  2.19it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.19it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.19it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.19it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.20it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.19it/s] 49%|████▉     | 36/73 [00:16<00:16,  2.20it/s] 51%|█████     | 37/73 [00:16<00:16,  2.20it/s] 52%|█████▏    | 38/73 [00:17<00:15,  2.20it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.20it/s] 55%|█████▍    | 40/73 [00:17<00:15,  2.20it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.20it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.20it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.20it/s] 60%|██████    | 44/73 [00:19<00:13,  2.20it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.20it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.20it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.20it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.20it/s] 67%|██████▋   | 49/73 [00:22<00:10,  2.20it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.20it/s] 70%|██████▉   | 51/73 [00:22<00:10,  2.20it/s] 71%|███████   | 52/73 [00:23<00:09,  2.20it/s] 73%|███████▎  | 53/73 [00:23<00:09,  2.18it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.18it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.18it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.19it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.19it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.19it/s] 81%|████████  | 59/73 [00:26<00:06,  2.19it/s] 82%|████████▏ | 60/73 [00:27<00:05,  2.20it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.20it/s] 85%|████████▍ | 62/73 [00:27<00:05,  2.20it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.20it/s] 88%|████████▊ | 64/73 [00:28<00:04,  2.20it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.20it/s] 90%|█████████ | 66/73 [00:29<00:03,  2.20it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.20it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.20it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.20it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.20it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.20it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.21it/s]100%|██████████| 73/73 [00:32<00:00,  2.64it/s]100%|██████████| 73/73 [00:33<00:00,  2.21it/s]
***** eval metrics *****
  eval_accuracy               =     0.7007
  eval_loss                   =     1.6644
  eval_model_preparation_time =     0.0029
  eval_runtime                = 0:00:33.91
  eval_samples                =        580
  eval_samples_per_second     =     17.101
  eval_steps_per_second       =      2.152
  perplexity                  =     5.2825
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-19024
11/15/2024 13:59:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 13:59:09 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:16,  4.27it/s]  4%|▍         | 3/73 [00:00<00:23,  3.01it/s]  5%|▌         | 4/73 [00:01<00:26,  2.64it/s]  7%|▋         | 5/73 [00:01<00:27,  2.46it/s]  8%|▊         | 6/73 [00:02<00:28,  2.35it/s] 10%|▉         | 7/73 [00:02<00:28,  2.28it/s] 11%|█         | 8/73 [00:03<00:28,  2.25it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.23it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.21it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.20it/s] 16%|█▋        | 12/73 [00:05<00:27,  2.20it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.19it/s] 19%|█▉        | 14/73 [00:05<00:26,  2.19it/s] 21%|██        | 15/73 [00:06<00:26,  2.19it/s] 22%|██▏       | 16/73 [00:06<00:26,  2.19it/s] 23%|██▎       | 17/73 [00:07<00:25,  2.16it/s] 25%|██▍       | 18/73 [00:07<00:25,  2.17it/s] 26%|██▌       | 19/73 [00:08<00:24,  2.17it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.18it/s] 29%|██▉       | 21/73 [00:09<00:23,  2.18it/s] 30%|███       | 22/73 [00:09<00:23,  2.19it/s] 32%|███▏      | 23/73 [00:10<00:22,  2.19it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.17it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.18it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.19it/s] 37%|███▋      | 27/73 [00:11<00:21,  2.19it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.19it/s] 40%|███▉      | 29/73 [00:12<00:20,  2.19it/s] 41%|████      | 30/73 [00:13<00:19,  2.19it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.19it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.19it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.19it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.19it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.19it/s] 49%|████▉     | 36/73 [00:16<00:16,  2.19it/s] 51%|█████     | 37/73 [00:16<00:16,  2.19it/s] 52%|█████▏    | 38/73 [00:16<00:15,  2.19it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.19it/s] 55%|█████▍    | 40/73 [00:17<00:15,  2.20it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.20it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.20it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.19it/s] 60%|██████    | 44/73 [00:19<00:13,  2.20it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.20it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.20it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.20it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.20it/s] 67%|██████▋   | 49/73 [00:21<00:10,  2.20it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.20it/s] 70%|██████▉   | 51/73 [00:22<00:10,  2.20it/s] 71%|███████   | 52/73 [00:23<00:09,  2.20it/s] 73%|███████▎  | 53/73 [00:23<00:09,  2.20it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.20it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.19it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.20it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.20it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.20it/s] 81%|████████  | 59/73 [00:26<00:06,  2.20it/s] 82%|████████▏ | 60/73 [00:26<00:05,  2.20it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.20it/s] 85%|████████▍ | 62/73 [00:27<00:05,  2.20it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.20it/s] 88%|████████▊ | 64/73 [00:28<00:04,  2.20it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.20it/s] 90%|█████████ | 66/73 [00:29<00:03,  2.20it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.19it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.20it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.20it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.20it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.16it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.18it/s]100%|██████████| 73/73 [00:32<00:00,  2.61it/s]100%|██████████| 73/73 [00:32<00:00,  2.22it/s]
***** eval metrics *****
  eval_accuracy               =     0.7059
  eval_loss                   =     1.6379
  eval_model_preparation_time =      0.003
  eval_runtime                = 0:00:33.80
  eval_samples                =        580
  eval_samples_per_second     =     17.155
  eval_steps_per_second       =      2.159
  perplexity                  =     5.1441
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-20213
11/15/2024 13:59:52 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 13:59:55 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:17,  4.04it/s]  4%|▍         | 3/73 [00:00<00:24,  2.87it/s]  5%|▌         | 4/73 [00:01<00:27,  2.50it/s]  7%|▋         | 5/73 [00:01<00:28,  2.35it/s]  8%|▊         | 6/73 [00:02<00:29,  2.30it/s] 10%|▉         | 7/73 [00:02<00:29,  2.26it/s] 11%|█         | 8/73 [00:03<00:28,  2.24it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.22it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.22it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.21it/s] 16%|█▋        | 12/73 [00:05<00:27,  2.21it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.20it/s] 19%|█▉        | 14/73 [00:06<00:26,  2.20it/s] 21%|██        | 15/73 [00:06<00:26,  2.20it/s] 22%|██▏       | 16/73 [00:06<00:26,  2.19it/s] 23%|██▎       | 17/73 [00:07<00:25,  2.19it/s] 25%|██▍       | 18/73 [00:07<00:25,  2.19it/s] 26%|██▌       | 19/73 [00:08<00:24,  2.19it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.19it/s] 29%|██▉       | 21/73 [00:09<00:23,  2.18it/s] 30%|███       | 22/73 [00:09<00:23,  2.18it/s] 32%|███▏      | 23/73 [00:10<00:22,  2.18it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.16it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.16it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.17it/s] 37%|███▋      | 27/73 [00:12<00:21,  2.18it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.19it/s] 40%|███▉      | 29/73 [00:12<00:20,  2.19it/s] 41%|████      | 30/73 [00:13<00:19,  2.19it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.19it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.19it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.19it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.19it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.19it/s] 49%|████▉     | 36/73 [00:16<00:16,  2.20it/s] 51%|█████     | 37/73 [00:16<00:16,  2.19it/s] 52%|█████▏    | 38/73 [00:17<00:15,  2.20it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.19it/s] 55%|█████▍    | 40/73 [00:17<00:15,  2.20it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.19it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.20it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.20it/s] 60%|██████    | 44/73 [00:19<00:13,  2.20it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.20it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.17it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.18it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.18it/s] 67%|██████▋   | 49/73 [00:22<00:10,  2.19it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.19it/s] 70%|██████▉   | 51/73 [00:22<00:10,  2.19it/s] 71%|███████   | 52/73 [00:23<00:09,  2.19it/s] 73%|███████▎  | 53/73 [00:23<00:09,  2.19it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.19it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.19it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.20it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.19it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.20it/s] 81%|████████  | 59/73 [00:26<00:06,  2.19it/s] 82%|████████▏ | 60/73 [00:27<00:05,  2.20it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.19it/s] 85%|████████▍ | 62/73 [00:27<00:05,  2.20it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.19it/s] 88%|████████▊ | 64/73 [00:28<00:04,  2.20it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.19it/s] 90%|█████████ | 66/73 [00:29<00:03,  2.19it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.19it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.19it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.19it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.19it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.19it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.21it/s]100%|██████████| 73/73 [00:32<00:00,  2.64it/s]100%|██████████| 73/73 [00:33<00:00,  2.21it/s]
***** eval metrics *****
  eval_accuracy               =     0.7053
  eval_loss                   =     1.6499
  eval_model_preparation_time =     0.0028
  eval_runtime                = 0:00:33.94
  eval_samples                =        580
  eval_samples_per_second     =     17.086
  eval_steps_per_second       =       2.15
  perplexity                  =     5.2066
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-21402
11/15/2024 14:00:37 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 14:00:40 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:17,  4.13it/s]  4%|▍         | 3/73 [00:00<00:24,  2.84it/s]  5%|▌         | 4/73 [00:01<00:27,  2.52it/s]  7%|▋         | 5/73 [00:01<00:28,  2.37it/s]  8%|▊         | 6/73 [00:02<00:29,  2.28it/s] 10%|▉         | 7/73 [00:02<00:29,  2.22it/s] 11%|█         | 8/73 [00:03<00:29,  2.19it/s] 12%|█▏        | 9/73 [00:03<00:29,  2.17it/s] 14%|█▎        | 10/73 [00:04<00:29,  2.15it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.15it/s] 16%|█▋        | 12/73 [00:05<00:28,  2.14it/s] 18%|█▊        | 13/73 [00:05<00:28,  2.14it/s] 19%|█▉        | 14/73 [00:06<00:27,  2.14it/s] 21%|██        | 15/73 [00:06<00:27,  2.14it/s] 22%|██▏       | 16/73 [00:07<00:26,  2.14it/s] 23%|██▎       | 17/73 [00:07<00:26,  2.14it/s] 25%|██▍       | 18/73 [00:08<00:25,  2.14it/s] 26%|██▌       | 19/73 [00:08<00:25,  2.14it/s] 27%|██▋       | 20/73 [00:08<00:25,  2.12it/s] 29%|██▉       | 21/73 [00:09<00:24,  2.12it/s] 30%|███       | 22/73 [00:09<00:23,  2.14it/s] 32%|███▏      | 23/73 [00:10<00:23,  2.15it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.16it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.17it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.14it/s] 37%|███▋      | 27/73 [00:12<00:21,  2.15it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.17it/s] 40%|███▉      | 29/73 [00:13<00:20,  2.17it/s] 41%|████      | 30/73 [00:13<00:19,  2.18it/s] 42%|████▏     | 31/73 [00:14<00:19,  2.18it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.19it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.17it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.18it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.18it/s] 49%|████▉     | 36/73 [00:16<00:16,  2.18it/s] 51%|█████     | 37/73 [00:16<00:16,  2.19it/s] 52%|█████▏    | 38/73 [00:17<00:16,  2.19it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.19it/s] 55%|█████▍    | 40/73 [00:18<00:15,  2.19it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.19it/s] 58%|█████▊    | 42/73 [00:19<00:14,  2.19it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.19it/s] 60%|██████    | 44/73 [00:19<00:13,  2.20it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.20it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.20it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.20it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.20it/s] 67%|██████▋   | 49/73 [00:22<00:10,  2.20it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.20it/s] 70%|██████▉   | 51/73 [00:23<00:10,  2.19it/s] 71%|███████   | 52/73 [00:23<00:09,  2.19it/s] 73%|███████▎  | 53/73 [00:24<00:09,  2.19it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.19it/s] 75%|███████▌  | 55/73 [00:25<00:08,  2.19it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.20it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.20it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.19it/s] 81%|████████  | 59/73 [00:26<00:06,  2.16it/s] 82%|████████▏ | 60/73 [00:27<00:06,  2.13it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.12it/s] 85%|████████▍ | 62/73 [00:28<00:05,  2.14it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.15it/s] 88%|████████▊ | 64/73 [00:29<00:04,  2.16it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.16it/s] 90%|█████████ | 66/73 [00:30<00:03,  2.17it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.17it/s] 93%|█████████▎| 68/73 [00:31<00:02,  2.17it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.17it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.17it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.17it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.19it/s]100%|██████████| 73/73 [00:33<00:00,  2.62it/s]100%|██████████| 73/73 [00:33<00:00,  2.19it/s]
***** eval metrics *****
  eval_accuracy               =     0.7059
  eval_loss                   =     1.6486
  eval_model_preparation_time =     0.0027
  eval_runtime                = 0:00:34.23
  eval_samples                =        580
  eval_samples_per_second     =     16.943
  eval_steps_per_second       =      2.132
  perplexity                  =     5.1997
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-22591
11/15/2024 14:01:23 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 14:01:26 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:17,  4.04it/s]  4%|▍         | 3/73 [00:00<00:24,  2.87it/s]  5%|▌         | 4/73 [00:01<00:28,  2.42it/s]  7%|▋         | 5/73 [00:01<00:29,  2.32it/s]  8%|▊         | 6/73 [00:02<00:29,  2.28it/s] 10%|▉         | 7/73 [00:02<00:29,  2.25it/s] 11%|█         | 8/73 [00:03<00:29,  2.24it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.23it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.20it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.19it/s] 16%|█▋        | 12/73 [00:05<00:27,  2.20it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.20it/s] 19%|█▉        | 14/73 [00:06<00:26,  2.19it/s] 21%|██        | 15/73 [00:06<00:26,  2.18it/s] 22%|██▏       | 16/73 [00:07<00:26,  2.17it/s] 23%|██▎       | 17/73 [00:07<00:26,  2.12it/s] 25%|██▍       | 18/73 [00:07<00:25,  2.14it/s] 26%|██▌       | 19/73 [00:08<00:25,  2.16it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.18it/s] 29%|██▉       | 21/73 [00:09<00:23,  2.18it/s] 30%|███       | 22/73 [00:09<00:23,  2.19it/s] 32%|███▏      | 23/73 [00:10<00:22,  2.19it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.20it/s] 34%|███▍      | 25/73 [00:11<00:21,  2.20it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.20it/s] 37%|███▋      | 27/73 [00:12<00:20,  2.20it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.19it/s] 40%|███▉      | 29/73 [00:12<00:20,  2.19it/s] 41%|████      | 30/73 [00:13<00:19,  2.17it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.17it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.18it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.19it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.20it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.20it/s] 49%|████▉     | 36/73 [00:16<00:16,  2.20it/s] 51%|█████     | 37/73 [00:16<00:16,  2.20it/s] 52%|█████▏    | 38/73 [00:17<00:15,  2.20it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.20it/s] 55%|█████▍    | 40/73 [00:17<00:14,  2.20it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.21it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.21it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.20it/s] 60%|██████    | 44/73 [00:19<00:13,  2.21it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.19it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.19it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.20it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.20it/s] 67%|██████▋   | 49/73 [00:22<00:10,  2.20it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.20it/s] 70%|██████▉   | 51/73 [00:22<00:09,  2.20it/s] 71%|███████   | 52/73 [00:23<00:09,  2.20it/s] 73%|███████▎  | 53/73 [00:23<00:09,  2.20it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.20it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.20it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.18it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.18it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.19it/s] 81%|████████  | 59/73 [00:26<00:06,  2.19it/s] 82%|████████▏ | 60/73 [00:27<00:05,  2.20it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.20it/s] 85%|████████▍ | 62/73 [00:27<00:04,  2.20it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.20it/s] 88%|████████▊ | 64/73 [00:28<00:04,  2.20it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.20it/s] 90%|█████████ | 66/73 [00:29<00:03,  2.20it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.20it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.20it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.21it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.21it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.20it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.22it/s]100%|██████████| 73/73 [00:32<00:00,  2.65it/s]100%|██████████| 73/73 [00:32<00:00,  2.21it/s]
***** eval metrics *****
  eval_accuracy               =     0.7118
  eval_loss                   =      1.609
  eval_model_preparation_time =     0.0028
  eval_runtime                = 0:00:33.92
  eval_samples                =        580
  eval_samples_per_second     =     17.098
  eval_steps_per_second       =      2.152
  perplexity                  =     4.9977
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-2378
11/15/2024 14:02:09 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 14:02:12 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:16,  4.30it/s]  4%|▍         | 3/73 [00:00<00:23,  3.03it/s]  5%|▌         | 4/73 [00:01<00:25,  2.66it/s]  7%|▋         | 5/73 [00:01<00:27,  2.48it/s]  8%|▊         | 6/73 [00:02<00:28,  2.36it/s] 10%|▉         | 7/73 [00:02<00:28,  2.30it/s] 11%|█         | 8/73 [00:03<00:28,  2.26it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.24it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.22it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.21it/s] 16%|█▋        | 12/73 [00:05<00:27,  2.20it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.20it/s] 19%|█▉        | 14/73 [00:05<00:26,  2.20it/s] 21%|██        | 15/73 [00:06<00:26,  2.20it/s] 22%|██▏       | 16/73 [00:06<00:25,  2.20it/s] 23%|██▎       | 17/73 [00:07<00:25,  2.20it/s] 25%|██▍       | 18/73 [00:07<00:24,  2.20it/s] 26%|██▌       | 19/73 [00:08<00:24,  2.19it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.19it/s] 29%|██▉       | 21/73 [00:09<00:23,  2.20it/s] 30%|███       | 22/73 [00:09<00:23,  2.17it/s] 32%|███▏      | 23/73 [00:10<00:23,  2.17it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.18it/s] 34%|███▍      | 25/73 [00:10<00:21,  2.19it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.19it/s] 37%|███▋      | 27/73 [00:11<00:20,  2.20it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.20it/s] 40%|███▉      | 29/73 [00:12<00:20,  2.20it/s] 41%|████      | 30/73 [00:13<00:19,  2.18it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.19it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.19it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.19it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.20it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.20it/s] 49%|████▉     | 36/73 [00:15<00:16,  2.20it/s] 51%|█████     | 37/73 [00:16<00:16,  2.20it/s] 52%|█████▏    | 38/73 [00:16<00:15,  2.20it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.20it/s] 55%|█████▍    | 40/73 [00:17<00:14,  2.20it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.20it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.20it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.16it/s] 60%|██████    | 44/73 [00:19<00:13,  2.17it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.18it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.19it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.19it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.19it/s] 67%|██████▋   | 49/73 [00:21<00:10,  2.20it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.20it/s] 70%|██████▉   | 51/73 [00:22<00:10,  2.20it/s] 71%|███████   | 52/73 [00:23<00:09,  2.20it/s] 73%|███████▎  | 53/73 [00:23<00:09,  2.20it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.20it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.20it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.20it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.20it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.20it/s] 81%|████████  | 59/73 [00:26<00:06,  2.20it/s] 82%|████████▏ | 60/73 [00:26<00:05,  2.20it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.20it/s] 85%|████████▍ | 62/73 [00:27<00:04,  2.20it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.20it/s] 88%|████████▊ | 64/73 [00:28<00:04,  2.21it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.20it/s] 90%|█████████ | 66/73 [00:29<00:03,  2.20it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.20it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.20it/s] 95%|█████████▍| 69/73 [00:30<00:01,  2.20it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.20it/s] 97%|█████████▋| 71/73 [00:31<00:00,  2.20it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.21it/s]100%|██████████| 73/73 [00:32<00:00,  2.65it/s]100%|██████████| 73/73 [00:32<00:00,  2.22it/s]
***** eval metrics *****
  eval_accuracy               =     0.5672
  eval_loss                   =     2.6948
  eval_model_preparation_time =     0.0038
  eval_runtime                = 0:00:33.73
  eval_samples                =        580
  eval_samples_per_second     =     17.193
  eval_steps_per_second       =      2.164
  perplexity                  =     14.803
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-23780
11/15/2024 14:02:54 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 14:02:57 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:17,  4.11it/s]  4%|▍         | 3/73 [00:00<00:24,  2.86it/s]  5%|▌         | 4/73 [00:01<00:27,  2.49it/s]  7%|▋         | 5/73 [00:01<00:28,  2.36it/s]  8%|▊         | 6/73 [00:02<00:29,  2.30it/s] 10%|▉         | 7/73 [00:02<00:29,  2.26it/s] 11%|█         | 8/73 [00:03<00:28,  2.24it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.23it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.21it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.21it/s] 16%|█▋        | 12/73 [00:05<00:27,  2.21it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.20it/s] 19%|█▉        | 14/73 [00:06<00:26,  2.20it/s] 21%|██        | 15/73 [00:06<00:26,  2.20it/s] 22%|██▏       | 16/73 [00:06<00:26,  2.18it/s] 23%|██▎       | 17/73 [00:07<00:26,  2.15it/s] 25%|██▍       | 18/73 [00:07<00:25,  2.16it/s] 26%|██▌       | 19/73 [00:08<00:24,  2.17it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.18it/s] 29%|██▉       | 21/73 [00:09<00:23,  2.19it/s] 30%|███       | 22/73 [00:09<00:23,  2.19it/s] 32%|███▏      | 23/73 [00:10<00:22,  2.19it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.20it/s] 34%|███▍      | 25/73 [00:11<00:21,  2.20it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.20it/s] 37%|███▋      | 27/73 [00:11<00:20,  2.20it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.18it/s] 40%|███▉      | 29/73 [00:12<00:20,  2.18it/s] 41%|████      | 30/73 [00:13<00:19,  2.16it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.17it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.18it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.18it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.19it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.19it/s] 49%|████▉     | 36/73 [00:16<00:16,  2.19it/s] 51%|█████     | 37/73 [00:16<00:16,  2.20it/s] 52%|█████▏    | 38/73 [00:17<00:15,  2.20it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.20it/s] 55%|█████▍    | 40/73 [00:17<00:14,  2.20it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.20it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.20it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.20it/s] 60%|██████    | 44/73 [00:19<00:13,  2.20it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.18it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.19it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.19it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.19it/s] 67%|██████▋   | 49/73 [00:22<00:10,  2.20it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.20it/s] 70%|██████▉   | 51/73 [00:22<00:10,  2.20it/s] 71%|███████   | 52/73 [00:23<00:09,  2.20it/s] 73%|███████▎  | 53/73 [00:23<00:09,  2.20it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.20it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.20it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.17it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.17it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.18it/s] 81%|████████  | 59/73 [00:26<00:06,  2.19it/s] 82%|████████▏ | 60/73 [00:27<00:05,  2.19it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.19it/s] 85%|████████▍ | 62/73 [00:27<00:05,  2.20it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.20it/s] 88%|████████▊ | 64/73 [00:28<00:04,  2.20it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.20it/s] 90%|█████████ | 66/73 [00:29<00:03,  2.20it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.20it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.18it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.19it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.19it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.19it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.21it/s]100%|██████████| 73/73 [00:32<00:00,  2.64it/s]100%|██████████| 73/73 [00:33<00:00,  2.21it/s]
***** eval metrics *****
  eval_accuracy               =     0.7134
  eval_loss                   =     1.6005
  eval_model_preparation_time =     0.0027
  eval_runtime                = 0:00:33.93
  eval_samples                =        580
  eval_samples_per_second     =     17.092
  eval_steps_per_second       =      2.151
  perplexity                  =     4.9557
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-3567
11/15/2024 14:03:39 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 14:03:42 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:16,  4.23it/s]  4%|▍         | 3/73 [00:00<00:23,  2.99it/s]  5%|▌         | 4/73 [00:01<00:26,  2.62it/s]  7%|▋         | 5/73 [00:01<00:27,  2.45it/s]  8%|▊         | 6/73 [00:02<00:28,  2.35it/s] 10%|▉         | 7/73 [00:02<00:28,  2.28it/s] 11%|█         | 8/73 [00:03<00:28,  2.25it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.22it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.19it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.19it/s] 16%|█▋        | 12/73 [00:05<00:27,  2.19it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.18it/s] 19%|█▉        | 14/73 [00:06<00:27,  2.18it/s] 21%|██        | 15/73 [00:06<00:26,  2.18it/s] 22%|██▏       | 16/73 [00:06<00:26,  2.18it/s] 23%|██▎       | 17/73 [00:07<00:25,  2.18it/s] 25%|██▍       | 18/73 [00:07<00:25,  2.18it/s] 26%|██▌       | 19/73 [00:08<00:24,  2.17it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.17it/s] 29%|██▉       | 21/73 [00:09<00:24,  2.15it/s] 30%|███       | 22/73 [00:09<00:23,  2.15it/s] 32%|███▏      | 23/73 [00:10<00:23,  2.16it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.17it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.17it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.18it/s] 37%|███▋      | 27/73 [00:12<00:21,  2.18it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.16it/s] 40%|███▉      | 29/73 [00:12<00:20,  2.16it/s] 41%|████      | 30/73 [00:13<00:19,  2.17it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.17it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.18it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.18it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.18it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.18it/s] 49%|████▉     | 36/73 [00:16<00:16,  2.19it/s] 51%|█████     | 37/73 [00:16<00:16,  2.19it/s] 52%|█████▏    | 38/73 [00:17<00:15,  2.19it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.19it/s] 55%|█████▍    | 40/73 [00:17<00:15,  2.16it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.17it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.18it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.18it/s] 60%|██████    | 44/73 [00:19<00:13,  2.19it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.19it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.19it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.19it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.20it/s] 67%|██████▋   | 49/73 [00:22<00:10,  2.20it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.20it/s] 70%|██████▉   | 51/73 [00:22<00:10,  2.19it/s] 71%|███████   | 52/73 [00:23<00:09,  2.20it/s] 73%|███████▎  | 53/73 [00:23<00:09,  2.20it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.20it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.20it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.20it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.20it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.20it/s] 81%|████████  | 59/73 [00:26<00:06,  2.19it/s] 82%|████████▏ | 60/73 [00:27<00:05,  2.18it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.18it/s] 85%|████████▍ | 62/73 [00:28<00:05,  2.19it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.18it/s] 88%|████████▊ | 64/73 [00:28<00:04,  2.16it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.17it/s] 90%|█████████ | 66/73 [00:29<00:03,  2.18it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.18it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.19it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.19it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.19it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.19it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.21it/s]100%|██████████| 73/73 [00:32<00:00,  2.64it/s]100%|██████████| 73/73 [00:33<00:00,  2.21it/s]
***** eval metrics *****
  eval_accuracy               =     0.6145
  eval_loss                   =     2.3825
  eval_model_preparation_time =     0.0041
  eval_runtime                = 0:00:33.95
  eval_samples                =        580
  eval_samples_per_second     =     17.081
  eval_steps_per_second       =       2.15
  perplexity                  =    10.8315
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-4756
11/15/2024 14:04:25 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 14:04:28 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:17,  4.09it/s]  4%|▍         | 3/73 [00:00<00:24,  2.85it/s]  5%|▌         | 4/73 [00:01<00:27,  2.49it/s]  7%|▋         | 5/73 [00:01<00:28,  2.36it/s]  8%|▊         | 6/73 [00:02<00:29,  2.30it/s] 10%|▉         | 7/73 [00:02<00:29,  2.27it/s] 11%|█         | 8/73 [00:03<00:28,  2.25it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.23it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.21it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.21it/s] 16%|█▋        | 12/73 [00:05<00:27,  2.21it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.21it/s] 19%|█▉        | 14/73 [00:06<00:26,  2.20it/s] 21%|██        | 15/73 [00:06<00:26,  2.20it/s] 22%|██▏       | 16/73 [00:06<00:25,  2.19it/s] 23%|██▎       | 17/73 [00:07<00:25,  2.20it/s] 25%|██▍       | 18/73 [00:07<00:24,  2.20it/s] 26%|██▌       | 19/73 [00:08<00:24,  2.20it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.19it/s] 29%|██▉       | 21/73 [00:09<00:23,  2.19it/s] 30%|███       | 22/73 [00:09<00:23,  2.19it/s] 32%|███▏      | 23/73 [00:10<00:22,  2.19it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.16it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.17it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.18it/s] 37%|███▋      | 27/73 [00:11<00:21,  2.19it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.19it/s] 40%|███▉      | 29/73 [00:12<00:20,  2.20it/s] 41%|████      | 30/73 [00:13<00:19,  2.20it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.20it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.20it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.20it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.20it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.20it/s] 49%|████▉     | 36/73 [00:16<00:16,  2.20it/s] 51%|█████     | 37/73 [00:16<00:16,  2.20it/s] 52%|█████▏    | 38/73 [00:16<00:15,  2.20it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.20it/s] 55%|█████▍    | 40/73 [00:17<00:14,  2.20it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.20it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.20it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.20it/s] 60%|██████    | 44/73 [00:19<00:13,  2.20it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.20it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.20it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.18it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.19it/s] 67%|██████▋   | 49/73 [00:21<00:10,  2.19it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.20it/s] 70%|██████▉   | 51/73 [00:22<00:10,  2.20it/s] 71%|███████   | 52/73 [00:23<00:09,  2.20it/s] 73%|███████▎  | 53/73 [00:23<00:09,  2.20it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.20it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.20it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.20it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.20it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.20it/s] 81%|████████  | 59/73 [00:26<00:06,  2.20it/s] 82%|████████▏ | 60/73 [00:26<00:05,  2.20it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.20it/s] 85%|████████▍ | 62/73 [00:27<00:04,  2.20it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.20it/s] 88%|████████▊ | 64/73 [00:28<00:04,  2.20it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.20it/s] 90%|█████████ | 66/73 [00:29<00:03,  2.20it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.20it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.20it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.20it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.20it/s] 97%|█████████▋| 71/73 [00:31<00:00,  2.20it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.21it/s]100%|██████████| 73/73 [00:32<00:00,  2.65it/s]100%|██████████| 73/73 [00:32<00:00,  2.22it/s]
***** eval metrics *****
  eval_accuracy               =     0.6311
  eval_loss                   =     2.2117
  eval_model_preparation_time =     0.0029
  eval_runtime                = 0:00:33.81
  eval_samples                =        580
  eval_samples_per_second     =     17.152
  eval_steps_per_second       =      2.159
  perplexity                  =     9.1308
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-5945
11/15/2024 14:05:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 14:05:13 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:16,  4.29it/s]  4%|▍         | 3/73 [00:00<00:23,  3.03it/s]  5%|▌         | 4/73 [00:01<00:25,  2.66it/s]  7%|▋         | 5/73 [00:01<00:27,  2.48it/s]  8%|▊         | 6/73 [00:02<00:28,  2.38it/s] 10%|▉         | 7/73 [00:02<00:28,  2.31it/s] 11%|█         | 8/73 [00:03<00:28,  2.27it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.25it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.23it/s] 15%|█▌        | 11/73 [00:04<00:27,  2.22it/s] 16%|█▋        | 12/73 [00:05<00:27,  2.21it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.21it/s] 19%|█▉        | 14/73 [00:05<00:26,  2.21it/s] 21%|██        | 15/73 [00:06<00:26,  2.20it/s] 22%|██▏       | 16/73 [00:06<00:25,  2.20it/s] 23%|██▎       | 17/73 [00:07<00:25,  2.20it/s] 25%|██▍       | 18/73 [00:07<00:24,  2.20it/s] 26%|██▌       | 19/73 [00:08<00:24,  2.20it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.20it/s] 29%|██▉       | 21/73 [00:09<00:23,  2.20it/s] 30%|███       | 22/73 [00:09<00:23,  2.17it/s] 32%|███▏      | 23/73 [00:10<00:23,  2.17it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.18it/s] 34%|███▍      | 25/73 [00:10<00:21,  2.19it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.19it/s] 37%|███▋      | 27/73 [00:11<00:20,  2.19it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.20it/s] 40%|███▉      | 29/73 [00:12<00:20,  2.20it/s] 41%|████      | 30/73 [00:13<00:19,  2.20it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.20it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.20it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.19it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.18it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.18it/s] 49%|████▉     | 36/73 [00:15<00:16,  2.19it/s] 51%|█████     | 37/73 [00:16<00:16,  2.19it/s] 52%|█████▏    | 38/73 [00:16<00:15,  2.19it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.19it/s] 55%|█████▍    | 40/73 [00:17<00:15,  2.20it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.20it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.20it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.20it/s] 60%|██████    | 44/73 [00:19<00:13,  2.20it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.20it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.20it/s] 64%|██████▍   | 47/73 [00:20<00:11,  2.20it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.20it/s] 67%|██████▋   | 49/73 [00:21<00:10,  2.20it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.20it/s] 70%|██████▉   | 51/73 [00:22<00:10,  2.20it/s] 71%|███████   | 52/73 [00:23<00:09,  2.20it/s] 73%|███████▎  | 53/73 [00:23<00:09,  2.20it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.20it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.20it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.20it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.20it/s] 79%|███████▉  | 58/73 [00:25<00:06,  2.20it/s] 81%|████████  | 59/73 [00:26<00:06,  2.20it/s] 82%|████████▏ | 60/73 [00:26<00:05,  2.20it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.20it/s] 85%|████████▍ | 62/73 [00:27<00:05,  2.20it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.20it/s] 88%|████████▊ | 64/73 [00:28<00:04,  2.20it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.20it/s] 90%|█████████ | 66/73 [00:29<00:03,  2.20it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.20it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.20it/s] 95%|█████████▍| 69/73 [00:30<00:01,  2.20it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.20it/s] 97%|█████████▋| 71/73 [00:31<00:00,  2.20it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.21it/s]100%|██████████| 73/73 [00:32<00:00,  2.64it/s]100%|██████████| 73/73 [00:32<00:00,  2.22it/s]
***** eval metrics *****
  eval_accuracy               =     0.6421
  eval_loss                   =     2.1157
  eval_model_preparation_time =     0.0034
  eval_runtime                = 0:00:33.70
  eval_samples                =        580
  eval_samples_per_second     =     17.209
  eval_steps_per_second       =      2.166
  perplexity                  =     8.2955
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-7134
11/15/2024 14:05:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 14:05:59 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:17,  4.09it/s]  4%|▍         | 3/73 [00:00<00:24,  2.88it/s]  5%|▌         | 4/73 [00:01<00:27,  2.51it/s]  7%|▋         | 5/73 [00:01<00:29,  2.33it/s]  8%|▊         | 6/73 [00:02<00:29,  2.24it/s] 10%|▉         | 7/73 [00:02<00:30,  2.15it/s] 11%|█         | 8/73 [00:03<00:30,  2.13it/s] 12%|█▏        | 9/73 [00:03<00:30,  2.12it/s] 14%|█▎        | 10/73 [00:04<00:29,  2.12it/s] 15%|█▌        | 11/73 [00:04<00:29,  2.11it/s] 16%|█▋        | 12/73 [00:05<00:28,  2.12it/s] 18%|█▊        | 13/73 [00:05<00:28,  2.11it/s] 19%|█▉        | 14/73 [00:06<00:27,  2.11it/s] 21%|██        | 15/73 [00:06<00:27,  2.11it/s] 22%|██▏       | 16/73 [00:07<00:27,  2.11it/s] 23%|██▎       | 17/73 [00:07<00:26,  2.11it/s] 25%|██▍       | 18/73 [00:08<00:26,  2.09it/s] 26%|██▌       | 19/73 [00:08<00:25,  2.09it/s] 27%|██▋       | 20/73 [00:09<00:25,  2.10it/s] 29%|██▉       | 21/73 [00:09<00:24,  2.09it/s] 30%|███       | 22/73 [00:10<00:24,  2.09it/s] 32%|███▏      | 23/73 [00:10<00:23,  2.09it/s] 33%|███▎      | 24/73 [00:11<00:23,  2.10it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.10it/s] 36%|███▌      | 26/73 [00:11<00:22,  2.10it/s] 37%|███▋      | 27/73 [00:12<00:21,  2.10it/s] 38%|███▊      | 28/73 [00:12<00:21,  2.05it/s] 40%|███▉      | 29/73 [00:13<00:21,  2.04it/s] 41%|████      | 30/73 [00:13<00:21,  2.05it/s] 42%|████▏     | 31/73 [00:14<00:20,  2.05it/s] 44%|████▍     | 32/73 [00:14<00:19,  2.06it/s] 45%|████▌     | 33/73 [00:15<00:19,  2.06it/s] 47%|████▋     | 34/73 [00:15<00:18,  2.06it/s] 48%|████▊     | 35/73 [00:16<00:18,  2.06it/s] 49%|████▉     | 36/73 [00:16<00:17,  2.06it/s] 51%|█████     | 37/73 [00:17<00:17,  2.06it/s] 52%|█████▏    | 38/73 [00:17<00:16,  2.07it/s] 53%|█████▎    | 39/73 [00:18<00:16,  2.07it/s] 55%|█████▍    | 40/73 [00:18<00:15,  2.07it/s] 56%|█████▌    | 41/73 [00:19<00:15,  2.07it/s] 58%|█████▊    | 42/73 [00:19<00:14,  2.07it/s] 59%|█████▉    | 43/73 [00:20<00:14,  2.07it/s] 60%|██████    | 44/73 [00:20<00:13,  2.08it/s] 62%|██████▏   | 45/73 [00:21<00:13,  2.08it/s] 63%|██████▎   | 46/73 [00:21<00:12,  2.08it/s] 64%|██████▍   | 47/73 [00:22<00:12,  2.08it/s] 66%|██████▌   | 48/73 [00:22<00:11,  2.08it/s] 67%|██████▋   | 49/73 [00:23<00:11,  2.08it/s] 68%|██████▊   | 50/73 [00:23<00:11,  2.09it/s] 70%|██████▉   | 51/73 [00:24<00:10,  2.09it/s] 71%|███████   | 52/73 [00:24<00:10,  2.09it/s] 73%|███████▎  | 53/73 [00:25<00:09,  2.09it/s] 74%|███████▍  | 54/73 [00:25<00:09,  2.07it/s] 75%|███████▌  | 55/73 [00:26<00:08,  2.08it/s] 77%|███████▋  | 56/73 [00:26<00:08,  2.08it/s] 78%|███████▊  | 57/73 [00:26<00:07,  2.09it/s] 79%|███████▉  | 58/73 [00:27<00:07,  2.09it/s] 81%|████████  | 59/73 [00:27<00:06,  2.09it/s] 82%|████████▏ | 60/73 [00:28<00:06,  2.10it/s] 84%|████████▎ | 61/73 [00:28<00:05,  2.10it/s] 85%|████████▍ | 62/73 [00:29<00:05,  2.10it/s] 86%|████████▋ | 63/73 [00:29<00:04,  2.10it/s] 88%|████████▊ | 64/73 [00:30<00:04,  2.11it/s] 89%|████████▉ | 65/73 [00:30<00:03,  2.11it/s] 90%|█████████ | 66/73 [00:31<00:03,  2.11it/s] 92%|█████████▏| 67/73 [00:31<00:02,  2.11it/s] 93%|█████████▎| 68/73 [00:32<00:02,  2.11it/s] 95%|█████████▍| 69/73 [00:32<00:01,  2.11it/s] 96%|█████████▌| 70/73 [00:33<00:01,  2.11it/s] 97%|█████████▋| 71/73 [00:33<00:00,  2.11it/s] 99%|█████████▊| 72/73 [00:34<00:00,  2.13it/s]100%|██████████| 73/73 [00:34<00:00,  2.56it/s]100%|██████████| 73/73 [00:34<00:00,  2.11it/s]
***** eval metrics *****
  eval_accuracy               =     0.6512
  eval_loss                   =     2.0505
  eval_model_preparation_time =      0.004
  eval_runtime                = 0:00:35.50
  eval_samples                =        580
  eval_samples_per_second     =     16.336
  eval_steps_per_second       =      2.056
  perplexity                  =     7.7721
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-8323
11/15/2024 14:06:42 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 14:06:45 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:16,  4.20it/s]  4%|▍         | 3/73 [00:00<00:24,  2.89it/s]  5%|▌         | 4/73 [00:01<00:26,  2.58it/s]  7%|▋         | 5/73 [00:01<00:27,  2.43it/s]  8%|▊         | 6/73 [00:02<00:28,  2.35it/s] 10%|▉         | 7/73 [00:02<00:28,  2.29it/s] 11%|█         | 8/73 [00:03<00:28,  2.25it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.24it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.22it/s] 15%|█▌        | 11/73 [00:04<00:27,  2.22it/s] 16%|█▋        | 12/73 [00:05<00:27,  2.20it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.20it/s] 19%|█▉        | 14/73 [00:05<00:26,  2.20it/s] 21%|██        | 15/73 [00:06<00:26,  2.20it/s] 22%|██▏       | 16/73 [00:06<00:25,  2.21it/s] 23%|██▎       | 17/73 [00:07<00:25,  2.20it/s] 25%|██▍       | 18/73 [00:07<00:24,  2.20it/s] 26%|██▌       | 19/73 [00:08<00:24,  2.20it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.20it/s] 29%|██▉       | 21/73 [00:09<00:23,  2.20it/s] 30%|███       | 22/73 [00:09<00:23,  2.20it/s] 32%|███▏      | 23/73 [00:10<00:23,  2.17it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.18it/s] 34%|███▍      | 25/73 [00:11<00:21,  2.18it/s] 36%|███▌      | 26/73 [00:11<00:21,  2.19it/s] 37%|███▋      | 27/73 [00:11<00:21,  2.19it/s] 38%|███▊      | 28/73 [00:12<00:20,  2.19it/s] 40%|███▉      | 29/73 [00:12<00:20,  2.20it/s] 41%|████      | 30/73 [00:13<00:19,  2.20it/s] 42%|████▏     | 31/73 [00:13<00:19,  2.20it/s] 44%|████▍     | 32/73 [00:14<00:18,  2.20it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.20it/s] 47%|████▋     | 34/73 [00:15<00:17,  2.18it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.19it/s] 49%|████▉     | 36/73 [00:16<00:16,  2.20it/s] 51%|█████     | 37/73 [00:16<00:16,  2.20it/s] 52%|█████▏    | 38/73 [00:16<00:15,  2.20it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.20it/s] 55%|█████▍    | 40/73 [00:17<00:14,  2.20it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.20it/s] 58%|█████▊    | 42/73 [00:18<00:14,  2.21it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.20it/s] 60%|██████    | 44/73 [00:19<00:13,  2.21it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.20it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.21it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.20it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.21it/s] 67%|██████▋   | 49/73 [00:21<00:10,  2.21it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.21it/s] 70%|██████▉   | 51/73 [00:22<00:09,  2.20it/s] 71%|███████   | 52/73 [00:23<00:09,  2.21it/s] 73%|███████▎  | 53/73 [00:23<00:09,  2.21it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.21it/s] 75%|███████▌  | 55/73 [00:24<00:08,  2.20it/s] 77%|███████▋  | 56/73 [00:25<00:08,  2.07it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.11it/s] 79%|███████▉  | 58/73 [00:26<00:07,  2.13it/s] 81%|████████  | 59/73 [00:26<00:06,  2.15it/s] 82%|████████▏ | 60/73 [00:27<00:05,  2.17it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.18it/s] 85%|████████▍ | 62/73 [00:27<00:05,  2.19it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.19it/s] 88%|████████▊ | 64/73 [00:28<00:04,  2.20it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.20it/s] 90%|█████████ | 66/73 [00:29<00:03,  2.20it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.20it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.20it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.20it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.20it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.17it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.19it/s]100%|██████████| 73/73 [00:32<00:00,  2.62it/s]100%|██████████| 73/73 [00:32<00:00,  2.21it/s]
***** eval metrics *****
  eval_accuracy               =     0.6459
  eval_loss                   =     2.0406
  eval_model_preparation_time =     0.0026
  eval_runtime                = 0:00:33.83
  eval_samples                =        580
  eval_samples_per_second     =     17.142
  eval_steps_per_second       =      2.158
  perplexity                  =     7.6948
/nfs/gdata/fkoerner/norsid_lang_exps/checkpoint-9512
11/15/2024 14:07:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 14:07:31 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:17,  4.10it/s]  4%|▍         | 3/73 [00:00<00:24,  2.89it/s]  5%|▌         | 4/73 [00:01<00:27,  2.51it/s]  7%|▋         | 5/73 [00:01<00:28,  2.36it/s]  8%|▊         | 6/73 [00:02<00:29,  2.31it/s] 10%|▉         | 7/73 [00:02<00:29,  2.27it/s] 11%|█         | 8/73 [00:03<00:28,  2.24it/s] 12%|█▏        | 9/73 [00:03<00:28,  2.23it/s] 14%|█▎        | 10/73 [00:04<00:28,  2.22it/s] 15%|█▌        | 11/73 [00:04<00:28,  2.21it/s] 16%|█▋        | 12/73 [00:05<00:27,  2.21it/s] 18%|█▊        | 13/73 [00:05<00:27,  2.20it/s] 19%|█▉        | 14/73 [00:06<00:26,  2.20it/s] 21%|██        | 15/73 [00:06<00:26,  2.20it/s] 22%|██▏       | 16/73 [00:06<00:26,  2.19it/s] 23%|██▎       | 17/73 [00:07<00:25,  2.19it/s] 25%|██▍       | 18/73 [00:07<00:25,  2.19it/s] 26%|██▌       | 19/73 [00:08<00:24,  2.19it/s] 27%|██▋       | 20/73 [00:08<00:24,  2.18it/s] 29%|██▉       | 21/73 [00:09<00:24,  2.15it/s] 30%|███       | 22/73 [00:09<00:23,  2.16it/s] 32%|███▏      | 23/73 [00:10<00:23,  2.17it/s] 33%|███▎      | 24/73 [00:10<00:22,  2.18it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.15it/s] 36%|███▌      | 26/73 [00:11<00:23,  2.02it/s] 37%|███▋      | 27/73 [00:12<00:22,  2.07it/s] 38%|███▊      | 28/73 [00:12<00:21,  2.05it/s] 40%|███▉      | 29/73 [00:13<00:21,  2.03it/s] 41%|████      | 30/73 [00:13<00:20,  2.07it/s] 42%|████▏     | 31/73 [00:14<00:20,  2.09it/s] 44%|████▍     | 32/73 [00:14<00:19,  2.10it/s] 45%|████▌     | 33/73 [00:14<00:18,  2.12it/s] 47%|████▋     | 34/73 [00:15<00:18,  2.15it/s] 48%|████▊     | 35/73 [00:15<00:17,  2.16it/s] 49%|████▉     | 36/73 [00:16<00:17,  2.17it/s] 51%|█████     | 37/73 [00:16<00:16,  2.18it/s] 52%|█████▏    | 38/73 [00:17<00:16,  2.19it/s] 53%|█████▎    | 39/73 [00:17<00:15,  2.19it/s] 55%|█████▍    | 40/73 [00:18<00:15,  2.20it/s] 56%|█████▌    | 41/73 [00:18<00:14,  2.20it/s] 58%|█████▊    | 42/73 [00:19<00:14,  2.20it/s] 59%|█████▉    | 43/73 [00:19<00:13,  2.18it/s] 60%|██████    | 44/73 [00:20<00:13,  2.19it/s] 62%|██████▏   | 45/73 [00:20<00:12,  2.19it/s] 63%|██████▎   | 46/73 [00:20<00:12,  2.19it/s] 64%|██████▍   | 47/73 [00:21<00:11,  2.19it/s] 66%|██████▌   | 48/73 [00:21<00:11,  2.20it/s] 67%|██████▋   | 49/73 [00:22<00:10,  2.20it/s] 68%|██████▊   | 50/73 [00:22<00:10,  2.20it/s] 70%|██████▉   | 51/73 [00:23<00:10,  2.18it/s] 71%|███████   | 52/73 [00:23<00:09,  2.16it/s] 73%|███████▎  | 53/73 [00:24<00:09,  2.17it/s] 74%|███████▍  | 54/73 [00:24<00:08,  2.18it/s] 75%|███████▌  | 55/73 [00:25<00:08,  2.19it/s] 77%|███████▋  | 56/73 [00:25<00:07,  2.19it/s] 78%|███████▊  | 57/73 [00:25<00:07,  2.19it/s] 79%|███████▉  | 58/73 [00:26<00:06,  2.20it/s] 81%|████████  | 59/73 [00:26<00:06,  2.20it/s] 82%|████████▏ | 60/73 [00:27<00:05,  2.20it/s] 84%|████████▎ | 61/73 [00:27<00:05,  2.20it/s] 85%|████████▍ | 62/73 [00:28<00:05,  2.20it/s] 86%|████████▋ | 63/73 [00:28<00:04,  2.20it/s] 88%|████████▊ | 64/73 [00:29<00:04,  2.20it/s] 89%|████████▉ | 65/73 [00:29<00:03,  2.20it/s] 90%|█████████ | 66/73 [00:30<00:03,  2.20it/s] 92%|█████████▏| 67/73 [00:30<00:02,  2.20it/s] 93%|█████████▎| 68/73 [00:30<00:02,  2.20it/s] 95%|█████████▍| 69/73 [00:31<00:01,  2.20it/s] 96%|█████████▌| 70/73 [00:31<00:01,  2.20it/s] 97%|█████████▋| 71/73 [00:32<00:00,  2.20it/s] 99%|█████████▊| 72/73 [00:32<00:00,  2.21it/s]100%|██████████| 73/73 [00:32<00:00,  2.64it/s]100%|██████████| 73/73 [00:33<00:00,  2.20it/s]
***** eval metrics *****
  eval_accuracy               =     0.6718
  eval_loss                   =     1.8778
  eval_model_preparation_time =     0.0028
  eval_runtime                = 0:00:34.17
  eval_samples                =        580
  eval_samples_per_second     =     16.973
  eval_steps_per_second       =      2.136
  perplexity                  =      6.539
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             71%|███████   | 52/73 [00:24<00:09,  2.12it/s] 73%|███████▎  | 53/73 [00:24<00:09,  2.12it/s] 74%|███████▍  | 54/73 [00:25<00:08,  2.12it/s] 75%|███████▌  | 55/73 [00:25<00:08,  2.12it/s] 77%|███████▋  | 56/73 [00:26<00:08,  2.12it/s] 78%|███████▊  | 57/73 [00:26<00:07,  2.12it/s] 79%|███████▉  | 58/73 [00:27<00:07,  2.12it/s] 81%|████████  | 59/73 [00:27<00:06,  2.09it/s] 82%|████████▏ | 60/73 [00:28<00:06,  2.10it/s] 84%|████████▎ | 61/73 [00:28<00:05,  2.10it/s] 85%|████████▍ | 62/73 [00:29<00:05,  2.11it/s] 86%|████████▋ | 63/73 [00:29<00:04,  2.11it/s] 88%|████████▊ | 64/73 [00:30<00:04,  2.12it/s] 89%|████████▉ | 65/73 [00:30<00:03,  2.12it/s] 90%|█████████ | 66/73 [00:31<00:03,  2.12it/s] 92%|█████████▏| 67/73 [00:31<00:02,  2.12it/s] 93%|█████████▎| 68/73 [00:31<00:02,  2.13it/s] 95%|█████████▍| 69/73 [00:32<00:01,  2.13it/s] 96%|█████████▌| 70/73 [00:32<00:01,  2.13it/s] 97%|█████████▋| 71/73 [00:33<00:00,  2.13it/s] 99%|█████████▊| 72/73 [00:33<00:00,  2.13it/s]100%|██████████| 73/73 [00:34<00:00,  2.56it/s]100%|██████████| 73/73 [00:34<00:00,  2.13it/s]
***** eval metrics *****
  eval_accuracy               =     0.6311
  eval_loss                   =     2.2117
  eval_model_preparation_time =     0.0028
  eval_runtime                = 0:00:35.24
  eval_samples                =        580
  eval_samples_per_second     =     16.458
  eval_steps_per_second       =      2.071
  perplexity                  =     9.1308
11/15/2024 13:53:13 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 13:53:16 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
Running tokenizer on every text in dataset:   0%|          | 0/43605 [00:00<?, ? examples/s]Running tokenizer on every text in dataset:   7%|▋         | 3000/43605 [00:00<00:01, 22244.18 examples/s]Running tokenizer on every text in dataset:  18%|█▊        | 8000/43605 [00:00<00:01, 31501.75 examples/s]Running tokenizer on every text in dataset:  28%|██▊       | 12000/43605 [00:00<00:01, 19044.43 examples/s]Running tokenizer on every text in dataset:  37%|███▋      | 16000/43605 [00:00<00:01, 23593.61 examples/s]Running tokenizer on every text in dataset:  46%|████▌     | 20000/43605 [00:00<00:00, 27082.46 examples/s]Running tokenizer on every text in dataset:  55%|█████▌    | 24000/43605 [00:01<00:01, 16488.93 examples/s]Running tokenizer on every text in dataset:  64%|██████▍   | 28000/43605 [00:01<00:00, 20005.28 examples/s]Running tokenizer on every text in dataset:  73%|███████▎  | 32000/43605 [00:01<00:00, 23086.83 examples/s]Running tokenizer on every text in dataset:  83%|████████▎ | 36000/43605 [00:01<00:00, 15526.45 examples/s]Running tokenizer on every text in dataset:  92%|█████████▏| 40000/43605 [00:02<00:00, 18841.95 examples/s]Running tokenizer on every text in dataset: 100%|██████████| 43605/43605 [00:02<00:00, 21463.70 examples/s]Running tokenizer on every text in dataset: 100%|██████████| 43605/43605 [00:02<00:00, 20392.25 examples/s]
Grouping texts in chunks of 1024:   0%|          | 0/43605 [00:00<?, ? examples/s]Grouping texts in chunks of 1024:   9%|▉         | 4000/43605 [00:00<00:01, 23128.55 examples/s]Grouping texts in chunks of 1024:  18%|█▊        | 8000/43605 [00:00<00:01, 24881.07 examples/s]Grouping texts in chunks of 1024:  25%|██▌       | 11000/43605 [00:00<00:02, 16020.12 examples/s]Grouping texts in chunks of 1024:  34%|███▍      | 15000/43605 [00:00<00:01, 19447.04 examples/s]Grouping texts in chunks of 1024:  44%|████▎     | 19000/43605 [00:00<00:01, 21855.98 examples/s]Grouping texts in chunks of 1024:  53%|█████▎    | 23000/43605 [00:01<00:00, 23454.55 examples/s]Grouping texts in chunks of 1024:  62%|██████▏   | 27000/43605 [00:01<00:00, 24450.17 examples/s]Grouping texts in chunks of 1024:  71%|███████   | 31000/43605 [00:01<00:00, 25149.64 examples/s]Grouping texts in chunks of 1024:  80%|████████  | 35000/43605 [00:01<00:00, 23952.32 examples/s]Grouping texts in chunks of 1024:  89%|████████▉ | 39000/43605 [00:01<00:00, 24519.76 examples/s]Grouping texts in chunks of 1024:  99%|█████████▊| 43000/43605 [00:01<00:00, 25310.52 examples/s]Grouping texts in chunks of 1024: 100%|██████████| 43605/43605 [00:01<00:00, 22808.32 examples/s]
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:17,  4.04it/s]  4%|▍         | 3/73 [00:00<00:24,  2.86it/s]  5%|▌         | 4/73 [00:01<00:27,  2.51it/s]  7%|▋         | 5/73 [00:01<00:29,  2.34it/s]  8%|▊         | 6/73 [00:02<00:29,  2.25it/s] 10%|▉         | 7/73 [00:02<00:30,  2.19it/s] 11%|█         | 8/73 [00:03<00:30,  2.16it/s] 12%|█▏        | 9/73 [00:03<00:29,  2.13it/s] 14%|█▎        | 10/73 [00:04<00:29,  2.12it/s] 15%|█▌        | 11/73 [00:04<00:29,  2.10it/s] 16%|█▋        | 12/73 [00:05<00:29,  2.10it/s] 18%|█▊        | 13/73 [00:05<00:28,  2.09it/s] 19%|█▉        | 14/73 [00:06<00:28,  2.09it/s] 21%|██        | 15/73 [00:06<00:27,  2.09it/s] 22%|██▏       | 16/73 [00:07<00:27,  2.09it/s] 23%|██▎       | 17/73 [00:07<00:27,  2.06it/s] 25%|██▍       | 18/73 [00:08<00:26,  2.06it/s] 26%|██▌       | 19/73 [00:08<00:26,  2.07it/s] 27%|██▋       | 20/73 [00:09<00:25,  2.08it/s] 29%|██▉       | 21/73 [00:09<00:24,  2.08it/s] 30%|███       | 22/73 [00:10<00:24,  2.08it/s] 32%|███▏      | 23/73 [00:10<00:23,  2.08it/s] 33%|███▎      | 24/73 [00:11<00:23,  2.09it/s] 34%|███▍      | 25/73 [00:11<00:23,  2.07it/s] 36%|███▌      | 26/73 [00:12<00:22,  2.08it/s] 37%|███▋      | 27/73 [00:12<00:22,  2.08it/s] 38%|███▊      | 28/73 [00:13<00:21,  2.09it/s] 40%|███▉      | 29/73 [00:13<00:21,  2.09it/s] 41%|████      | 30/73 [00:13<00:20,  2.10it/s] 42%|████▏     | 31/73 [00:14<00:20,  2.10it/s] 44%|████▍     | 32/73 [00:14<00:19,  2.10it/s] 45%|████▌     | 33/73 [00:15<00:19,  2.10it/s] 47%|████▋     | 34/73 [00:15<00:18,  2.10it/s] 48%|████▊     | 35/73 [00:16<00:18,  2.10it/s] 49%|████▉     | 36/73 [00:16<00:17,  2.11it/s] 51%|█████     | 37/73 [00:17<00:17,  2.11it/s] 52%|█████▏    | 38/73 [00:17<00:16,  2.11it/s] 53%|█████▎    | 39/73 [00:18<00:16,  2.11it/s] 55%|█████▍    | 40/73 [00:18<00:15,  2.11it/s] 56%|█████▌    | 41/73 [00:19<00:15,  2.11it/s] 58%|█████▊    | 42/73 [00:19<00:14,  2.11it/s] 59%|█████▉    | 43/73 [00:20<00:14,  2.11it/s] 60%|██████    | 44/73 [00:20<00:13,  2.12it/s] 62%|██████▏   | 45/73 [00:21<00:13,  2.11it/s] 63%|██████▎   | 46/73 [00:21<00:12,  2.12it/s] 64%|██████▍   | 47/73 [00:22<00:12,  2.11it/s] 66%|██████▌   | 48/73 [00:22<00:11,  2.12it/s] 67%|██████▋   | 49/73 [00:22<00:11,  2.12it/s] 68%|██████▊   | 50/73 [00:23<00:10,  2.12it/s] 70%|██████▉   | 51/73 [00:23<00:10,  2.12it/s] 71%|███████   | 52/73 [00:24<00:09,  2.12it/s] 73%|███████▎  | 53/73 [00:24<00:09,  2.09it/s] 74%|███████▍  | 54/73 [00:25<00:09,  2.09it/s] 75%|███████▌  | 55/73 [00:25<00:08,  2.10it/s] 77%|███████▋  | 56/73 [00:26<00:08,  2.11it/s] 78%|███████▊  | 57/73 [00:26<00:07,  2.11it/s] 79%|███████▉  | 58/73 [00:27<00:07,  2.12it/s] 81%|████████  | 59/73 [00:27<00:06,  2.12it/s] 82%|████████▏ | 60/73 [00:28<00:06,  2.12it/s] 84%|████████▎ | 61/73 [00:28<00:05,  2.12it/s] 85%|████████▍ | 62/73 [00:29<00:05,  2.12it/s] 86%|████████▋ | 63/73 [00:29<00:04,  2.12it/s] 88%|████████▊ | 64/73 [00:30<00:04,  2.13it/s] 89%|████████▉ | 65/73 [00:30<00:03,  2.12it/s] 90%|█████████ | 66/73 [00:30<00:03,  2.11it/s] 92%|█████████▏| 67/73 [00:31<00:02,  2.11it/s] 93%|█████████▎| 68/73 [00:31<00:02,  2.12it/s] 95%|█████████▍| 69/73 [00:32<00:01,  2.12it/s] 96%|█████████▌| 70/73 [00:32<00:01,  2.12it/s] 97%|█████████▋| 71/73 [00:33<00:00,  2.12it/s] 99%|█████████▊| 72/73 [00:33<00:00,  2.14it/s]100%|██████████| 73/73 [00:34<00:00,  2.57it/s]100%|██████████| 73/73 [00:34<00:00,  2.13it/s]
***** eval metrics *****
  eval_accuracy               =     0.6421
  eval_loss                   =     2.1157
  eval_model_preparation_time =     0.0028
  eval_runtime                = 0:00:35.18
  eval_samples                =        580
  eval_samples_per_second     =     16.484
  eval_steps_per_second       =      2.075
  perplexity                  =     8.2955
11/15/2024 13:54:04 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 13:54:07 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
Running tokenizer on every text in dataset:   0%|          | 0/43605 [00:00<?, ? examples/s]Running tokenizer on every text in dataset:   7%|▋         | 3000/43605 [00:00<00:01, 21985.87 examples/s]Running tokenizer on every text in dataset:  16%|█▌        | 7000/43605 [00:00<00:01, 29099.26 examples/s]Running tokenizer on every text in dataset:  23%|██▎       | 10000/43605 [00:00<00:02, 15564.87 examples/s]Running tokenizer on every text in dataset:  32%|███▏      | 14000/43605 [00:00<00:01, 20050.47 examples/s]Running tokenizer on every text in dataset:  41%|████▏     | 18000/43605 [00:00<00:01, 24386.03 examples/s]Running tokenizer on every text in dataset:  50%|█████     | 22000/43605 [00:01<00:01, 19219.63 examples/s]Running tokenizer on every text in dataset:  60%|█████▉    | 26000/43605 [00:01<00:00, 22630.92 examples/s]Running tokenizer on every text in dataset:  69%|██████▉   | 30000/43605 [00:01<00:00, 25754.12 examples/s]Running tokenizer on every text in dataset:  78%|███████▊  | 34000/43605 [00:01<00:00, 18999.68 examples/s]Running tokenizer on every text in dataset:  87%|████████▋ | 38000/43605 [00:01<00:00, 21532.53 examples/s]Running tokenizer on every text in dataset:  96%|█████████▋| 42000/43605 [00:01<00:00, 24132.96 examples/s]Running tokenizer on every text in dataset: 100%|██████████| 43605/43605 [00:01<00:00, 21885.59 examples/s]
Grouping texts in chunks of 1024:   0%|          | 0/43605 [00:00<?, ? examples/s]Grouping texts in chunks of 1024:   7%|▋         | 3000/43605 [00:00<00:01, 22470.85 examples/s]Grouping texts in chunks of 1024:  16%|█▌        | 7000/43605 [00:00<00:01, 25054.40 examples/s]Grouping texts in chunks of 1024:  25%|██▌       | 11000/43605 [00:00<00:02, 15523.34 examples/s]Grouping texts in chunks of 1024:  34%|███▍      | 15000/43605 [00:00<00:01, 18898.02 examples/s]Grouping texts in chunks of 1024:  44%|████▎     | 19000/43605 [00:00<00:01, 21322.97 examples/s]Grouping texts in chunks of 1024:  53%|█████▎    | 23000/43605 [00:01<00:00, 22979.15 examples/s]Grouping texts in chunks of 1024:  62%|██████▏   | 27000/43605 [00:01<00:00, 23900.26 examples/s]Grouping texts in chunks of 1024:  71%|███████   | 31000/43605 [00:01<00:00, 24771.04 examples/s]Grouping texts in chunks of 1024:  78%|███████▊  | 34000/43605 [00:01<00:00, 23703.31 examples/s]Grouping texts in chunks of 1024:  87%|████████▋ | 38000/43605 [00:01<00:00, 24444.08 examples/s]Grouping texts in chunks of 1024:  96%|█████████▋| 42000/43605 [00:01<00:00, 25352.01 examples/s]Grouping texts in chunks of 1024: 100%|██████████| 43605/43605 [00:01<00:00, 22604.34 examples/s]
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:17,  4.03it/s]  4%|▍         | 3/73 [00:00<00:24,  2.85it/s]  5%|▌         | 4/73 [00:01<00:27,  2.50it/s]  7%|▋         | 5/73 [00:01<00:29,  2.33it/s]  8%|▊         | 6/73 [00:02<00:29,  2.24it/s] 10%|▉         | 7/73 [00:02<00:30,  2.17it/s] 11%|█         | 8/73 [00:03<00:30,  2.13it/s] 12%|█▏        | 9/73 [00:03<00:30,  2.11it/s] 14%|█▎        | 10/73 [00:04<00:29,  2.10it/s] 15%|█▌        | 11/73 [00:04<00:29,  2.09it/s] 16%|█▋        | 12/73 [00:05<00:29,  2.09it/s] 18%|█▊        | 13/73 [00:05<00:28,  2.09it/s] 19%|█▉        | 14/73 [00:06<00:28,  2.08it/s] 21%|██        | 15/73 [00:06<00:27,  2.08it/s] 22%|██▏       | 16/73 [00:07<00:27,  2.08it/s] 23%|██▎       | 17/73 [00:07<00:26,  2.08it/s] 25%|██▍       | 18/73 [00:08<00:26,  2.06it/s] 26%|██▌       | 19/73 [00:08<00:26,  2.06it/s] 27%|██▋       | 20/73 [00:09<00:25,  2.07it/s] 29%|██▉       | 21/73 [00:09<00:25,  2.08it/s] 30%|███       | 22/73 [00:10<00:24,  2.08it/s] 32%|███▏      | 23/73 [00:10<00:23,  2.08it/s] 33%|███▎      | 24/73 [00:11<00:23,  2.09it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.09it/s] 36%|███▌      | 26/73 [00:12<00:22,  2.09it/s] 37%|███▋      | 27/73 [00:12<00:21,  2.09it/s] 38%|███▊      | 28/73 [00:13<00:21,  2.10it/s] 40%|███▉      | 29/73 [00:13<00:20,  2.10it/s] 41%|████      | 30/73 [00:13<00:20,  2.10it/s] 42%|████▏     | 31/73 [00:14<00:19,  2.10it/s] 44%|████▍     | 32/73 [00:14<00:19,  2.10it/s] 45%|████▌     | 33/73 [00:15<00:19,  2.08it/s] 47%|████▋     | 34/73 [00:15<00:18,  2.09it/s] 48%|████▊     | 35/73 [00:16<00:18,  2.09it/s] 49%|████▉     | 36/73 [00:16<00:17,  2.10it/s] 51%|█████     | 37/73 [00:17<00:17,  2.10it/s] 52%|█████▏    | 38/73 [00:17<00:16,  2.07it/s] 53%|█████▎    | 39/73 [00:18<00:16,  2.08it/s] 55%|█████▍    | 40/73 [00:18<00:15,  2.09it/s] 56%|█████▌    | 41/73 [00:19<00:15,  2.09it/s] 58%|█████▊    | 42/73 [00:19<00:14,  2.10it/s] 59%|█████▉    | 43/73 [00:20<00:14,  2.10it/s] 60%|██████    | 44/73 [00:20<00:13,  2.11it/s] 62%|██████▏   | 45/73 [00:21<00:13,  2.11it/s] 63%|██████▎   | 46/73 [00:21<00:12,  2.12it/s] 64%|██████▍   | 47/73 [00:22<00:12,  2.12it/s] 66%|██████▌   | 48/73 [00:22<00:11,  2.12it/s] 67%|██████▋   | 49/73 [00:23<00:11,  2.11it/s] 68%|██████▊   | 50/73 [00:23<00:10,  2.11it/s] 70%|██████▉   | 51/73 [00:23<00:10,  2.11it/s] 71%|███████   | 52/73 [00:24<00:09,  2.11it/s] 73%|███████▎  | 53/73 [00:24<00:09,  2.11it/s] 74%|███████▍  | 54/73 [00:25<00:08,  2.11it/s] 75%|███████▌  | 55/73 [00:25<00:08,  2.11it/s] 77%|███████▋  | 56/73 [00:26<00:08,  2.12it/s] 78%|███████▊  | 57/73 [00:26<00:07,  2.12it/s] 79%|███████▉  | 58/73 [00:27<00:07,  2.12it/s] 81%|████████  | 59/73 [00:27<00:06,  2.12it/s] 82%|████████▏ | 60/73 [00:28<00:06,  2.12it/s] 84%|████████▎ | 61/73 [00:28<00:05,  2.12it/s] 85%|████████▍ | 62/73 [00:29<00:05,  2.12it/s] 86%|████████▋ | 63/73 [00:29<00:04,  2.10it/s] 88%|████████▊ | 64/73 [00:30<00:04,  2.11it/s] 89%|████████▉ | 65/73 [00:30<00:03,  2.08it/s] 90%|█████████ | 66/73 [00:31<00:03,  2.09it/s] 92%|█████████▏| 67/73 [00:31<00:02,  2.10it/s] 93%|█████████▎| 68/73 [00:32<00:02,  2.11it/s] 95%|█████████▍| 69/73 [00:32<00:01,  2.11it/s] 96%|█████████▌| 70/73 [00:32<00:01,  2.12it/s] 97%|█████████▋| 71/73 [00:33<00:00,  2.12it/s] 99%|█████████▊| 72/73 [00:33<00:00,  2.13it/s]100%|██████████| 73/73 [00:34<00:00,  2.56it/s]100%|██████████| 73/73 [00:34<00:00,  2.12it/s]
***** eval metrics *****
  eval_accuracy               =     0.6512
  eval_loss                   =     2.0505
  eval_model_preparation_time =     0.0041
  eval_runtime                = 0:00:35.31
  eval_samples                =        580
  eval_samples_per_second     =     16.425
  eval_steps_per_second       =      2.067
  perplexity                  =     7.7721
11/15/2024 13:54:54 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 13:54:57 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
Running tokenizer on every text in dataset:   0%|          | 0/43605 [00:00<?, ? examples/s]Running tokenizer on every text in dataset:   7%|▋         | 3000/43605 [00:00<00:01, 21638.64 examples/s]Running tokenizer on every text in dataset:  16%|█▌        | 7000/43605 [00:00<00:01, 30194.21 examples/s]Running tokenizer on every text in dataset:  28%|██▊       | 12000/43605 [00:00<00:01, 18766.64 examples/s]Running tokenizer on every text in dataset:  37%|███▋      | 16000/43605 [00:00<00:01, 22966.14 examples/s]Running tokenizer on every text in dataset:  46%|████▌     | 20000/43605 [00:00<00:00, 26754.30 examples/s]Running tokenizer on every text in dataset:  55%|█████▌    | 24000/43605 [00:01<00:00, 22703.73 examples/s]Running tokenizer on every text in dataset:  64%|██████▍   | 28000/43605 [00:01<00:00, 26289.17 examples/s]Running tokenizer on every text in dataset:  73%|███████▎  | 32000/43605 [00:01<00:00, 28803.73 examples/s]Running tokenizer on every text in dataset:  83%|████████▎ | 36000/43605 [00:01<00:00, 23323.00 examples/s]Running tokenizer on every text in dataset:  92%|█████████▏| 40000/43605 [00:01<00:00, 26178.61 examples/s]Running tokenizer on every text in dataset: 100%|██████████| 43605/43605 [00:01<00:00, 27294.10 examples/s]Running tokenizer on every text in dataset: 100%|██████████| 43605/43605 [00:01<00:00, 24878.73 examples/s]
Grouping texts in chunks of 1024:   0%|          | 0/43605 [00:00<?, ? examples/s]Grouping texts in chunks of 1024:   7%|▋         | 3000/43605 [00:00<00:01, 21460.24 examples/s]Grouping texts in chunks of 1024:  16%|█▌        | 7000/43605 [00:00<00:01, 24707.66 examples/s]Grouping texts in chunks of 1024:  25%|██▌       | 11000/43605 [00:00<00:01, 18382.15 examples/s]Grouping texts in chunks of 1024:  34%|███▍      | 15000/43605 [00:00<00:01, 21323.87 examples/s]Grouping texts in chunks of 1024:  44%|████▎     | 19000/43605 [00:00<00:01, 23294.44 examples/s]Grouping texts in chunks of 1024:  53%|█████▎    | 23000/43605 [00:01<00:00, 24232.04 examples/s]Grouping texts in chunks of 1024:  62%|██████▏   | 27000/43605 [00:01<00:00, 25160.51 examples/s]Grouping texts in chunks of 1024:  71%|███████   | 31000/43605 [00:01<00:00, 25821.78 examples/s]Grouping texts in chunks of 1024:  80%|████████  | 35000/43605 [00:01<00:00, 24523.04 examples/s]Grouping texts in chunks of 1024:  89%|████████▉ | 39000/43605 [00:01<00:00, 25377.17 examples/s]Grouping texts in chunks of 1024:  99%|█████████▊| 43000/43605 [00:01<00:00, 25726.92 examples/s]Grouping texts in chunks of 1024: 100%|██████████| 43605/43605 [00:01<00:00, 23806.80 examples/s]
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:18,  3.82it/s]  4%|▍         | 3/73 [00:01<00:25,  2.77it/s]  5%|▌         | 4/73 [00:01<00:27,  2.47it/s]  7%|▋         | 5/73 [00:01<00:29,  2.32it/s]  8%|▊         | 6/73 [00:02<00:29,  2.24it/s] 10%|▉         | 7/73 [00:02<00:30,  2.18it/s] 11%|█         | 8/73 [00:03<00:30,  2.15it/s] 12%|█▏        | 9/73 [00:03<00:30,  2.13it/s] 14%|█▎        | 10/73 [00:04<00:29,  2.12it/s] 15%|█▌        | 11/73 [00:04<00:29,  2.11it/s] 16%|█▋        | 12/73 [00:05<00:29,  2.10it/s] 18%|█▊        | 13/73 [00:05<00:28,  2.10it/s] 19%|█▉        | 14/73 [00:06<00:28,  2.09it/s] 21%|██        | 15/73 [00:06<00:27,  2.09it/s] 22%|██▏       | 16/73 [00:07<00:27,  2.09it/s] 23%|██▎       | 17/73 [00:07<00:26,  2.09it/s] 25%|██▍       | 18/73 [00:08<00:26,  2.08it/s] 26%|██▌       | 19/73 [00:08<00:25,  2.09it/s] 27%|██▋       | 20/73 [00:09<00:25,  2.09it/s] 29%|██▉       | 21/73 [00:09<00:24,  2.09it/s] 30%|███       | 22/73 [00:10<00:24,  2.10it/s] 32%|███▏      | 23/73 [00:10<00:23,  2.10it/s] 33%|███▎      | 24/73 [00:11<00:23,  2.10it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.10it/s] 36%|███▌      | 26/73 [00:12<00:22,  2.10it/s] 37%|███▋      | 27/73 [00:12<00:21,  2.10it/s] 38%|███▊      | 28/73 [00:12<00:21,  2.11it/s] 40%|███▉      | 29/73 [00:13<00:20,  2.10it/s] 41%|████      | 30/73 [00:13<00:20,  2.11it/s] 42%|████▏     | 31/73 [00:14<00:19,  2.10it/s] 44%|████▍     | 32/73 [00:14<00:19,  2.11it/s] 45%|████▌     | 33/73 [00:15<00:19,  2.11it/s] 47%|████▋     | 34/73 [00:15<00:18,  2.10it/s] 48%|████▊     | 35/73 [00:16<00:18,  2.10it/s] 49%|████▉     | 36/73 [00:16<00:17,  2.10it/s] 51%|█████     | 37/73 [00:17<00:17,  2.10it/s] 52%|█████▏    | 38/73 [00:17<00:16,  2.11it/s] 53%|█████▎    | 39/73 [00:18<00:16,  2.11it/s] 55%|█████▍    | 40/73 [00:18<00:15,  2.11it/s] 56%|█████▌    | 41/73 [00:19<00:15,  2.11it/s] 58%|█████▊    | 42/73 [00:19<00:14,  2.11it/s] 59%|█████▉    | 43/73 [00:20<00:14,  2.11it/s] 60%|██████    | 44/73 [00:20<00:13,  2.11it/s] 62%|██████▏   | 45/73 [00:21<00:13,  2.11it/s] 63%|██████▎   | 46/73 [00:21<00:12,  2.11it/s] 64%|██████▍   | 47/73 [00:21<00:12,  2.11it/s] 66%|██████▌   | 48/73 [00:22<00:11,  2.11it/s] 67%|██████▋   | 49/73 [00:22<00:11,  2.09it/s] 68%|██████▊   | 50/73 [00:23<00:10,  2.09it/s] 70%|██████▉   | 51/73 [00:23<00:10,  2.10it/s] 71%|███████   | 52/73 [00:24<00:09,  2.11it/s] 73%|███████▎  | 53/73 [00:24<00:09,  2.11it/s] 74%|███████▍  | 54/73 [00:25<00:09,  2.11it/s] 75%|███████▌  | 55/73 [00:25<00:08,  2.11it/s] 77%|███████▋  | 56/73 [00:26<00:08,  2.12it/s] 78%|███████▊  | 57/73 [00:26<00:07,  2.12it/s] 79%|███████▉  | 58/73 [00:27<00:07,  2.12it/s] 81%|████████  | 59/73 [00:27<00:06,  2.12it/s] 82%|████████▏ | 60/73 [00:28<00:06,  2.12it/s] 84%|████████▎ | 61/73 [00:28<00:05,  2.12it/s] 85%|████████▍ | 62/73 [00:29<00:05,  2.12it/s] 86%|████████▋ | 63/73 [00:29<00:04,  2.12it/s] 88%|████████▊ | 64/73 [00:30<00:04,  2.12it/s] 89%|████████▉ | 65/73 [00:30<00:03,  2.12it/s] 90%|█████████ | 66/73 [00:30<00:03,  2.11it/s] 92%|█████████▏| 67/73 [00:31<00:02,  2.11it/s] 93%|█████████▎| 68/73 [00:31<00:02,  2.12it/s] 95%|█████████▍| 69/73 [00:32<00:01,  2.12it/s] 96%|█████████▌| 70/73 [00:32<00:01,  2.12it/s] 97%|█████████▋| 71/73 [00:33<00:00,  2.12it/s] 99%|█████████▊| 72/73 [00:33<00:00,  2.14it/s]100%|██████████| 73/73 [00:34<00:00,  2.57it/s]100%|██████████| 73/73 [00:34<00:00,  2.13it/s]
***** eval metrics *****
  eval_accuracy               =     0.6459
  eval_loss                   =     2.0406
  eval_model_preparation_time =     0.0026
  eval_runtime                = 0:00:35.15
  eval_samples                =        580
  eval_samples_per_second     =     16.501
  eval_steps_per_second       =      2.077
  perplexity                  =     7.6948
11/15/2024 13:55:44 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2024 13:55:47 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
Running tokenizer on every text in dataset:   0%|          | 0/43605 [00:00<?, ? examples/s]Running tokenizer on every text in dataset:   7%|▋         | 3000/43605 [00:00<00:01, 22262.09 examples/s]Running tokenizer on every text in dataset:  16%|█▌        | 7000/43605 [00:00<00:01, 30556.20 examples/s]Running tokenizer on every text in dataset:  28%|██▊       | 12000/43605 [00:00<00:01, 16202.80 examples/s]Running tokenizer on every text in dataset:  37%|███▋      | 16000/43605 [00:00<00:01, 20859.72 examples/s]Running tokenizer on every text in dataset:  46%|████▌     | 20000/43605 [00:00<00:00, 24706.38 examples/s]Running tokenizer on every text in dataset:  55%|█████▌    | 24000/43605 [00:01<00:01, 19461.83 examples/s]Running tokenizer on every text in dataset:  64%|██████▍   | 28000/43605 [00:01<00:00, 22614.71 examples/s]Running tokenizer on every text in dataset:  73%|███████▎  | 32000/43605 [00:01<00:00, 25101.63 examples/s]Running tokenizer on every text in dataset:  83%|████████▎ | 36000/43605 [00:01<00:00, 18704.88 examples/s]Running tokenizer on every text in dataset:  92%|█████████▏| 40000/43605 [00:01<00:00, 21403.50 examples/s]Running tokenizer on every text in dataset: 100%|██████████| 43605/43605 [00:01<00:00, 23588.28 examples/s]Running tokenizer on every text in dataset: 100%|██████████| 43605/43605 [00:02<00:00, 21666.02 examples/s]
Grouping texts in chunks of 1024:   0%|          | 0/43605 [00:00<?, ? examples/s]Grouping texts in chunks of 1024:   7%|▋         | 3000/43605 [00:00<00:01, 22600.45 examples/s]Grouping texts in chunks of 1024:  16%|█▌        | 7000/43605 [00:00<00:01, 25316.83 examples/s]Grouping texts in chunks of 1024:  25%|██▌       | 11000/43605 [00:00<00:01, 17438.47 examples/s]Grouping texts in chunks of 1024:  34%|███▍      | 15000/43605 [00:00<00:01, 20537.04 examples/s]Grouping texts in chunks of 1024:  44%|████▎     | 19000/43605 [00:00<00:01, 22617.46 examples/s]Grouping texts in chunks of 1024:  53%|█████▎    | 23000/43605 [00:01<00:00, 24020.65 examples/s]Grouping texts in chunks of 1024:  62%|██████▏   | 27000/43605 [00:01<00:00, 24956.82 examples/s]Grouping texts in chunks of 1024:  71%|███████   | 31000/43605 [00:01<00:00, 25123.42 examples/s]Grouping texts in chunks of 1024:  80%|████████  | 35000/43605 [00:01<00:00, 23991.91 examples/s]Grouping texts in chunks of 1024:  89%|████████▉ | 39000/43605 [00:01<00:00, 24974.40 examples/s]Grouping texts in chunks of 1024:  99%|█████████▊| 43000/43605 [00:01<00:00, 25674.85 examples/s]Grouping texts in chunks of 1024: 100%|██████████| 43605/43605 [00:01<00:00, 23412.55 examples/s]
/mounts/Users/cisintern/fkoerner/.conda/envs/nor-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.
  warnings.warn(
/mounts/Users/cisintern/fkoerner/NorSID/experiments_layer_swap/lang_expert/norsid/../train_mlm.py:541: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:17,  4.06it/s]  4%|▍         | 3/73 [00:00<00:24,  2.87it/s]  5%|▌         | 4/73 [00:01<00:27,  2.52it/s]  7%|▋         | 5/73 [00:01<00:28,  2.35it/s]  8%|▊         | 6/73 [00:02<00:29,  2.26it/s] 10%|▉         | 7/73 [00:02<00:29,  2.20it/s] 11%|█         | 8/73 [00:03<00:29,  2.17it/s] 12%|█▏        | 9/73 [00:03<00:30,  2.13it/s] 14%|█▎        | 10/73 [00:04<00:29,  2.12it/s] 15%|█▌        | 11/73 [00:04<00:29,  2.11it/s] 16%|█▋        | 12/73 [00:05<00:28,  2.11it/s] 18%|█▊        | 13/73 [00:05<00:28,  2.10it/s] 19%|█▉        | 14/73 [00:06<00:28,  2.10it/s] 21%|██        | 15/73 [00:06<00:27,  2.10it/s] 22%|██▏       | 16/73 [00:07<00:27,  2.10it/s] 23%|██▎       | 17/73 [00:07<00:26,  2.09it/s] 25%|██▍       | 18/73 [00:08<00:26,  2.09it/s] 26%|██▌       | 19/73 [00:08<00:26,  2.07it/s] 27%|██▋       | 20/73 [00:09<00:25,  2.07it/s] 29%|██▉       | 21/73 [00:09<00:24,  2.08it/s] 30%|███       | 22/73 [00:10<00:24,  2.07it/s] 32%|███▏      | 23/73 [00:10<00:24,  2.08it/s] 33%|███▎      | 24/73 [00:11<00:23,  2.09it/s] 34%|███▍      | 25/73 [00:11<00:22,  2.09it/s] 36%|███▌      | 26/73 [00:12<00:22,  2.10it/s] 37%|███▋      | 27/73 [00:12<00:21,  2.10it/s] 38%|███▊      | 28/73 [00:12<00:21,  2.10it/s] 40%|███▉      | 29/73 [00:13<00:20,  2.10it/s] 41%|████      | 30/73 [00:13<00:20,  2.11it/s] 42%|████▏     | 31/73 [00:14<00:19,  2.11it/s] 44%|████▍     | 32/73 [00:14<00:19,  2.11it/s] 45%|████▌     | 33/73 [00:15<00:18,  2.11it/s] 47%|████▋     | 34/73 [00:15<00:18,  2.11it/s] 48%|████▊     | 35/73 [00:16<00:18,  2.11it/s] 49%|████▉     | 36/73 [00:16<00:17,  2.11it/s] 51%|█████     | 37/73 [00:17<00:17,  2.12it/s] 52%|█████▏    | 38/73 [00:17<00:16,  2.11it/s] 53%|█████▎    | 39/73 [00:18<00:16,  2.11it/s] 55%|█████▍    | 40/73 [00:18<00:15,  2.08it/s] 56%|█████▌    | 41/73 [00:19<00:15,  2.08it/s] 58%|█████▊    | 42/73 [00:19<00:14,  2.09it/s] 59%|█████▉    | 43/73 [00:20<00:14,  2.10it/s] 60%|██████    | 44/73 [00:20<00:13,  2.10it/s] 62%|██████▏   | 45/73 [00:21<00:13,  2.10it/s] 63%|██████▎   | 46/73 [00:21<00:12,  2.10it/s] 64%|██████▍   | 47/73 [00:21<00:12,  2.10it/s] 66%|██████▌   | 48/73 [00:22<00:11,  2.08it/s] 67%|██████▋   | 49/73 [00:22<00:11,  2.09it/s] 68%|██████▊   | 50/73 [00:23<00:10,  2.10it/s] 70%|██████▉   | 51/73 [00:23<00:10,  2.10it/s] 71%|███████   | 52/73 [00:24<00:09,  2.11it/s] 73%|███████▎  | 53/73 [00:24<00:09,  2.11it/s] 74%|███████▍  | 54/73 [00:25<00:08,  2.12it/s] 75%|███████▌  | 55/73 [00:25<00:08,  2.12it/s] 77%|███████▋  | 56/73 [00:26<00:08,  2.12it/s] 78%|███████▊  | 57/73 [00:26<00:07,  2.11it/s] 79%|███████▉  | 58/73 [00:27<00:07,  2.12it/s] 81%|████████  | 59/73 [00:27<00:06,  2.12it/s] 82%|████████▏ | 60/73 [00:28<00:06,  2.12it/s] 84%|████████▎ | 61/73 [00:28<00:05,  2.12it/s] 85%|████████▍ | 62/73 [00:29<00:05,  2.09it/s] 86%|████████▋ | 63/73 [00:29<00:04,  2.09it/s] 88%|████████▊ | 64/73 [00:30<00:04,  2.11it/s] 89%|████████▉ | 65/73 [00:30<00:03,  2.11it/s] 90%|█████████ | 66/73 [00:30<00:03,  2.12it/s] 92%|█████████▏| 67/73 [00:31<00:02,  2.12it/s] 93%|█████████▎| 68/73 [00:31<00:02,  2.12it/s] 95%|█████████▍| 69/73 [00:32<00:01,  2.12it/s] 96%|█████████▌| 70/73 [00:32<00:01,  2.13it/s] 97%|█████████▋| 71/73 [00:33<00:00,  2.13it/s] 99%|█████████▊| 72/73 [00:33<00:00,  2.14it/s]100%|██████████| 73/73 [00:34<00:00,  2.57it/s]100%|██████████| 73/73 [00:34<00:00,  2.13it/s]
***** eval metrics *****
  eval_accuracy               =     0.6718
  eval_loss                   =     1.8778
  eval_model_preparation_time =     0.0027
  eval_runtime                = 0:00:35.18
  eval_samples                =        580
  eval_samples_per_second     =     16.485
  eval_steps_per_second       =      2.075
  perplexity                  =      6.539
